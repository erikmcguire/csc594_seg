{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GH Copy of seg_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ov7iIFhCJNYj",
        "2z5zYcWdGjGg",
        "JkaNzZi5Nzvx",
        "xBa2drcPIuMX"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7iIFhCJNYj",
        "colab_type": "text"
      },
      "source": [
        "# Story Ending Generation Evaluations\n",
        "\n",
        "Erik S. McGuire\n",
        "\n",
        "CSC594-810-ADL, Winter 19-20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z5zYcWdGjGg",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF91x6rwGHzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIkYCg6fGDoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ipywidgets import *\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXhUVM2ML8Re",
        "colab_type": "text"
      },
      "source": [
        "> We must navigate to the main project folder in mounted My Drive. \n",
        "\n",
        "> Assumes the following structure:\n",
        "<pre>.\n",
        "├── content\n",
        "│   ├──drive                         # Mounted drive folder.\n",
        "│   │   └── My Drive                 # Mounted drive folder.\n",
        "│   │       └── CSC-594-ADL          # Main project folder.\n",
        "│   │           ├── datasets         # ConceptNet and ROCStories.\n",
        "│   │           ├── endings          # Correct and generated endings per model.\n",
        "│   │           ├── evals            # Evaluation results for stories and endings per model.\n",
        "│   │           ├── models           # Pretrained models, tokenizers, vocabulary, etc.\n",
        "│   │           ├── scripts          # Scripts for training and generation.\n",
        "│   │           └── stories          # Combined story bodies and generated endings per model.\n",
        "│   ├── sample_data                  # Default Colab folder.\n",
        "│   └── transformers                 # Installed from HuggingFace.\n",
        "└── ...\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCnQa5KrGFXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My Drive/csc594-ADL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkaNzZi5Nzvx",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Definitions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAwbbtiOSTdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bold_max(s): \n",
        "    \"\"\"\"Bold largest values.\"\"\" \n",
        "    if s.name != \"Models\":\n",
        "        is_max = s == s.max()\n",
        "    else:\n",
        "        is_max = s == False\n",
        "    return ['font-weight: bold' if v else '' for v in is_max]\n",
        "\n",
        "def ital_min(s):\n",
        "    \"\"\"\"Italicize smallest values.\"\"\" \n",
        "    if s.name != \"Models\":\n",
        "        is_min = s == s.min()\n",
        "    else:\n",
        "        is_min = s == False\n",
        "    return ['font-style: italic' if v else '' for v in is_min]\n",
        "\n",
        "def display_df_evals(model_type: str, eval_type: str, show_full: bool):\n",
        "    \"\"\"Display metrics results with emphasis on min/max vals.\"\"\"\n",
        "    print(f'Evaluations for {model_type}:')\n",
        "    print(\"\\n------------\\n\")\n",
        "    evals = {}\n",
        "    metrics = [('Corpus-level overlap', 'corpus_all.txt'), \n",
        "               ('Entity coreference', 'trunc_corefs.csv'), \n",
        "               ('Distinct', 'distinct_evals.txt'), \n",
        "               ('Skip-Thought', 'sg_skip_evals.txt'), \n",
        "               ('GloVe', 'sg_glove_evals.txt'), \n",
        "               ('Averaged truncated Skip/GloVe', 'o_skip_glove_evals.txt'), \n",
        "               ('Perplexity', 'ppl.csv'), \n",
        "               ('Flesch-Kincaid readability', 'joint_readability.txt')]\n",
        "    for metric in metrics:\n",
        "        if eval_type == metric[0]:\n",
        "            try:\n",
        "                evals[metric[0]] = pd.read_csv(f\"evals/{model_type}_{metric[1]}\", sep=',')\n",
        "            except:\n",
        "                if 'Distinct' in metric[0]:\n",
        "                    try:\n",
        "                        evals[metric[0]] = pd.read_csv(f\"evals/{model_type}_{'distinct_evals.txt'}\", \n",
        "                                                    sep=',')\n",
        "                    except:\n",
        "                        pass\n",
        "                elif 'Entity' in metric[0]:\n",
        "                    try:\n",
        "                        evals[metric[0]] = pd.read_csv(f\"evals/{model_type}_{'corefs.csv'}\", \n",
        "                                                    sep=',')\n",
        "                    except:\n",
        "                        pass\n",
        "                else:\n",
        "                    print(f\"{metric[0]} evaluations weren't found. \\n\")\n",
        "    if eval_type == 'Skip-Thought/GloVe':\n",
        "        try:\n",
        "            evals['GloVe'] = pd.DataFrame(evals['GloVe'].VectorExtremaCosineSimilarity)\n",
        "            skip_glove_evals_df = evals['GloVe'].join(evals['Skip-Thought'])\n",
        "            evals['Skip/GloVe'] = skip_glove_evals_df\n",
        "            del evals['Skip-Thought']\n",
        "            del evals['GloVe']\n",
        "        except:\n",
        "            print(f\"Skip-Thought/GloVe evaluations weren't found. \\n\")\n",
        "    for m, e in evals.items():\n",
        "        try:\n",
        "            if 'Entity' in m:\n",
        "                if not evals[m].columns.values[0] == \"Model\":\n",
        "                    evals[m].columns.values[0] = \"Model\"\n",
        "            if 'Perplexity' in m or 'Distinct' in m:\n",
        "                if 'Distinct' in m:\n",
        "                    evals[m] = evals[m].mean(axis=0)\n",
        "                if 'Perplexity' in m:\n",
        "                    ppl_df = evals[m]\n",
        "                if not show_full:\n",
        "                    evals[m] = evals[m].head()\n",
        "            if 'truncated' in m or 'readability' in m:\n",
        "                evals[m] = evals[m].mean()\n",
        "            print(f\"{m}:\\n\")\n",
        "            display(evals[m])\n",
        "            print(\"\\n------------\\n\")\n",
        "        except:\n",
        "            continue\n",
        "    try:\n",
        "        return ppl_df\n",
        "    except:\n",
        "        return pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBa2drcPIuMX",
        "colab_type": "text"
      },
      "source": [
        "## Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWb6q6aPFUUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_dfe = interactive(display_df_evals,\n",
        "                          model_type=Dropdown(options=[\n",
        "                                            ('Base', 'gpt2'),\n",
        "                                            ('Base to SCT', 'b_SCT'),\n",
        "                                            ('ConceptNet to SCT', 'cn_SCT'), #\n",
        "                                            ('CN to SCT to Sentiment', 'cn_SCT_sentiment'),\n",
        "                                            ('CN to Sentiment to SCT', 'cn_sentiment_SCT'),\n",
        "                                            ('ROC to SCT to Sentiment', 'roc1617_SCT_sentiment'),\n",
        "                                            ('ROC to Sentiment to SCT', 'roc1617_sentiment_SCT'),\n",
        "                                            ('ROC to SCT', 'roc1617_SCT')],\n",
        "                                            description=\"Model type\"),\n",
        "                          eval_type=Dropdown(options=['Corpus-level overlap', \n",
        "                                                      'Entity coreference', \n",
        "                                                      'Distinct', \n",
        "                                                      'Skip-Thought/GloVe', \n",
        "                                                      'Averaged truncated Skip/GloVe', \n",
        "                                                      'Perplexity', \n",
        "                                                      'Flesch-Kincaid readability'],\n",
        "                                             description=\"Metric\"),\n",
        "                          show_full=Checkbox(value=False, description=\"Show head ... tail\"))\n",
        "\n",
        "display_dfe.layout.height = \"450px\"\n",
        "display(display_dfe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1TGWMFxTGMR",
        "colab_type": "text"
      },
      "source": [
        "Display models' overlap and distinct average scores evaluated on endings generated for story prompts from ROCStories test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12EsPflf14FD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_df = pd.read_csv(f'evals/models_base_all.txt', sep=',')\n",
        "all_df.style.apply(bold_max).apply(ital_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaASyOWRfkGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ppl_all_df = pd.read_csv(f\"evals/models_ppl.txt\", sep=',')\n",
        "display(ppl_all_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_qM1VELdrGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_corefs = pd.read_csv(f\"evals/models_corefs.txt\", sep=',')\n",
        "display(all_corefs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeStmpMhIoYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(all_ease.drop(\"GoldEase\", axis=1).to_latex(index=False, float_format=\"%.3f\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yb0nQbrLitA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_ease = pd.read_csv(f\"evals/models_ease.txt\", sep=',')\n",
        "display(all_ease)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}