{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GH Copy of seg_visualization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5pWslYbMgpoc",
        "2z5zYcWdGjGg",
        "JkaNzZi5Nzvx",
        "LV48Tkclwf4l",
        "XT1UXICXw0xV",
        "OagNdmAqwxgl",
        "nsNosKa2G6qb",
        "MUX2gW_EPxj0",
        "HnI8UgAnE0OG",
        "fOkshGdxFYOR",
        "JKVf4LchQKWT",
        "o99WZ3K5e14y",
        "Bt4wTL5meHWT",
        "b8_vYNT7elHl"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pWslYbMgpoc",
        "colab_type": "text"
      },
      "source": [
        "# Story Ending Generation (Visualizations)\n",
        "\n",
        "Erik McGuire\n",
        "\n",
        "CSC594-810-ADL\n",
        "\n",
        "Winter 19-20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z5zYcWdGjGg",
        "colab_type": "text"
      },
      "source": [
        "## I. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF91x6rwGHzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8gDpTPF7aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Required over !pip install transformers for subclassing model/overriding methods.\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!pip install .\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdZfUtYUGB1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel, GPT2LMHeadModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIkYCg6fGDoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging, torch\n",
        "import torch.nn.functional as F # For softmax viz.\n",
        "import matplotlib.pyplot as plt # For alt. attention viz.\n",
        "from typing import Dict, List, Tuple\n",
        "from ipywidgets import * # Interface.\n",
        "import seaborn as sns # Attention viz.\n",
        "import pandas as pd # Data.\n",
        "import random # Choose random story for attentions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dIjKb_TT1Ae",
        "colab_type": "text"
      },
      "source": [
        "> We must navigate to the main project folder in mounted My Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCnQa5KrGFXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My Drive/csc594-ADL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ZBsPHB5THg",
        "colab_type": "text"
      },
      "source": [
        "> Assumes the following structure:\n",
        "<pre>.\n",
        "├── content\n",
        "│   ├──drive                         # Mounted drive folder.\n",
        "│   │   └── My Drive                 # Mounted drive folder.\n",
        "│   │       └── CSC-594-ADL          # Main project folder.\n",
        "│   │           ├── datasets         # ConceptNet and ROCStories.\n",
        "│   │           ├── endings          # Correct and generated endings per model.\n",
        "│   │           ├── evals            # Evaluation results for stories and endings per model.\n",
        "│   │           ├── models           # Pretrained models, tokenizers, vocabulary, etc.\n",
        "│   │           ├── scripts          # Scripts for training and generation.\n",
        "│   │           └── stories          # Combined story bodies and generated endings per model.\n",
        "│   ├── sample_data                  # Default Colab folder.\n",
        "│   └── transformers                 # Installed from HuggingFace.\n",
        "└── ...\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkaNzZi5Nzvx",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## II. Functions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV48Tkclwf4l",
        "colab_type": "text"
      },
      "source": [
        "#### Attention\n",
        "* Inspired by Krishan Subudhi's [code](https://krishansubudhi.github.io/deeplearning/2019/09/26/BertAttention.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F-nZ2fXwjic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_attns(prompt: str, ending: str, \n",
        "              model, tokenizer, \n",
        "              mname: str) -> Tuple[List[str], int, torch.Tensor]:\n",
        "    \"\"\"Get attention weights for story.\"\"\"\n",
        "    prompt_tokens = tokenizer.tokenize(prompt)\n",
        "    if not mname == 'gpt2':\n",
        "        end_tokens = tokenizer.tokenize(\"_delimiter_\" + ending)\n",
        "    else:\n",
        "        end_tokens = tokenizer.tokenize(ending)\n",
        "    pos_token = len(prompt_tokens)\n",
        "    in_tokens = prompt_tokens + end_tokens\n",
        "    in_tokens = list(map(lambda w: w.replace(\"Ġ\", \"\"), in_tokens))\n",
        "    ids = torch.tensor(tokenizer.convert_tokens_to_ids(in_tokens)).unsqueeze(0).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        output = model(ids)\n",
        "    attentions = torch.cat(output[-1]).cpu()\n",
        "    attentions = attentions.permute(2,1,0,3)\n",
        "    return in_tokens, pos_token, attentions\n",
        "\n",
        "def display_attns(in_tokens: list, attns: torch.Tensor, pos_token: int) -> None: \n",
        "    \"Displays multi-head attention weights for a token.\"\n",
        "    heads = len(attns[0])\n",
        "    cols = 2\n",
        "    rows = int(heads/cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize = (27, 27))\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    axes = axes.flat\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    print(f'\\nMultihead attention weights:')\n",
        "    for i, att in enumerate(attentions_pos):\n",
        "        sns.heatmap(att, vmin = 0, vmax = 1, ax = axes[i], xticklabels = in_tokens)\n",
        "        axes[i].set_title(f'Head #{i+1} ' )\n",
        "        axes[i].set_ylabel('Layers')\n",
        "        for tick in axes[i].get_xticklabels():\n",
        "                tick.set_rotation(45)\n",
        "                tick.set_fontsize(8) \n",
        "\n",
        "def display_per_head_attn(in_tokens: list, \n",
        "                                     attns: torch.Tensor, \n",
        "                                     pos_token: int, \n",
        "                                     head: int) -> None:\n",
        "    \"Given head number and position index, displays attention weights for a token.\"\n",
        "    head = head -1\n",
        "    fig, axes = plt.subplots(figsize = (20, 8))\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    print(f'\\nHeadwise attention weights:')\n",
        "    sns.heatmap(attentions_pos[head], \n",
        "                vmin = 0, vmax = 1, \n",
        "                xticklabels = in_tokens)\n",
        "    axes.set_title(f'Head #{head + 1} ' )\n",
        "    axes.set_ylabel('Layers')\n",
        "    for ix, tick in enumerate(axes.get_xticklabels()):\n",
        "            tick.set_rotation(55)\n",
        "            tick.set_fontsize(9) \n",
        "            if tick.get_text() == in_tokens[pos_token]:\n",
        "                try:\n",
        "                    if axes.get_xticklabels()[ix+1].get_text() == ca:\n",
        "                        if axes.get_xticklabels()[ix-1].get_text() == cp:\n",
        "                            tick.set_color(\"magenta\")\n",
        "                            tick.set_fontsize(10)\n",
        "                except:\n",
        "                    tick.set_color(\"magenta\")\n",
        "                    tick.set_fontsize(10)\n",
        "\n",
        "def display_per_layer_attn(in_tokens: list, \n",
        "                           attns: torch.Tensor, \n",
        "                           pos_token: int, \n",
        "                           layer: int) -> None:\n",
        "    \"Given layer number and position index, displays attention weights for a token.\"\n",
        "    layer -= 1\n",
        "    fig, axes = plt.subplots(figsize = (20, 8))\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    attns = attns.permute(0, 2, 1, 3)\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    print(f'\\nLayerwise attention weights:')\n",
        "    sns.heatmap(attentions_pos[layer], \n",
        "                vmin = 0, vmax = 1, \n",
        "                xticklabels = in_tokens)\n",
        "    axes.set_title(f'Layer #{layer + 1} ' )\n",
        "    axes.set_ylabel('Heads')\n",
        "    for ix, tick in enumerate(axes.get_xticklabels()):\n",
        "            tick.set_rotation(55)\n",
        "            tick.set_fontsize(9) \n",
        "            if tick.get_text() == in_tokens[pos_token]:\n",
        "                try:\n",
        "                    if axes.get_xticklabels()[ix+1].get_text() == ca:\n",
        "                        if axes.get_xticklabels()[ix-1].get_text() == cp:\n",
        "                            tick.set_color(\"magenta\")\n",
        "                            tick.set_fontsize(10)\n",
        "                except:\n",
        "                    tick.set_color(\"magenta\")\n",
        "                    tick.set_fontsize(10)\n",
        "\n",
        "def display_per_layer_per_head_attn(in_tokens: list, \n",
        "                                    attns: torch.Tensor, \n",
        "                                    pos_token: int, \n",
        "                                    layer: int, \n",
        "                                    head: int) -> None:\n",
        "    \"Given head, layer number and position index, displays attention weights for a token.\"\n",
        "    layer -= 1\n",
        "    head -= 1\n",
        "    fig, axes = plt.subplots(figsize = (20, 1))\n",
        "    attns = attns.permute(0, 2, 1, 3)\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    print(f'\\nLayerwise attention weights for given head:')\n",
        "    sns.heatmap(attentions_pos[layer][head].reshape(1, -1), \n",
        "                vmin = 0, vmax = 1, \n",
        "                xticklabels = in_tokens)\n",
        "    axes.set_title(f'Layer #{layer + 1} ' )\n",
        "    axes.set_ylabel(f'Head #{head + 1}')\n",
        "    for ix, tick in enumerate(axes.get_xticklabels()):\n",
        "            tick.set_rotation(55)\n",
        "            tick.set_fontsize(9) \n",
        "            if tick.get_text() == in_tokens[pos_token]:\n",
        "                try:\n",
        "                    if axes.get_xticklabels()[ix+1].get_text() == ca:\n",
        "                        if axes.get_xticklabels()[ix-1].get_text() == cp:\n",
        "                            tick.set_color(\"magenta\")\n",
        "                            tick.set_fontsize(10)\n",
        "                except:\n",
        "                    tick.set_color(\"magenta\")\n",
        "                    tick.set_fontsize(10)\n",
        "\n",
        "\n",
        "def display_per_head_attn_alt(in_tokens: list, \n",
        "                              attns: torch.Tensor, \n",
        "                              pos_token: int, \n",
        "                              head: int) -> None:\n",
        "    \"Given head number and position index, displays attention weights for a token.\"\n",
        "    head -= 1\n",
        "    fig, axes = plt.subplots(figsize = (20, 8))\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    print(f'\\nHeadwise attention weights:')\n",
        "    plt.imshow(attentions_pos[head])\n",
        "    plt.xticks(range(len(in_tokens)), in_tokens, rotation=45)\n",
        "    plt.title(f'Head #{head + 1} ' )\n",
        "    plt.ylabel('Layers')\n",
        "    plt.show()\n",
        "\n",
        "def display_avg_attn(in_tokens: list, attns: torch.Tensor, pos_token: int) -> None:\n",
        "    \"\"\"Display mean attention weights for token.\"\"\"\n",
        "    fig, axes = plt.subplots(figsize = (20, 8))\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    avg_attn = attentions_pos.mean(dim=0)\n",
        "    plt.title(f'\\nAverage attention weights')\n",
        "    sns.heatmap(avg_attn, vmin = 0, vmax = 1, xticklabels = in_tokens)\n",
        "    axes.set_ylabel('Layers')\n",
        "    plt.xticks(rotation=55, fontsize=8)\n",
        "    for ix, tick in enumerate(axes.get_xticklabels()):\n",
        "        if tick.get_text() == in_tokens[pos_token]:\n",
        "            try:\n",
        "                if axes.get_xticklabels()[ix+1].get_text() == ca:\n",
        "                    if axes.get_xticklabels()[ix-1].get_text() == cp:\n",
        "                        tick.set_color(\"magenta\")\n",
        "                        tick.set_fontsize(10)\n",
        "            except:\n",
        "                tick.set_color(\"magenta\")\n",
        "                tick.set_fontsize(10)\n",
        "    plt.show()\n",
        "\n",
        "def display_avg_attn_l(in_tokens: list, attns: torch.Tensor, pos_token: int) -> None:\n",
        "    \"\"\"Display mean attention weights for token.\"\"\"\n",
        "    fig, axes = plt.subplots(figsize = (20, 8))\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    attns = attns.permute(0, 2, 1, 3)\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    avg_attn = attentions_pos.mean(dim=0)\n",
        "    plt.title(f'\\nAverage attention weights (layers)')\n",
        "    sns.heatmap(avg_attn, vmin = 0, vmax = 1, xticklabels = in_tokens)\n",
        "    axes.set_ylabel('Heads')\n",
        "    plt.xticks(rotation=55, fontsize=8)\n",
        "    for ix, tick in enumerate(axes.get_xticklabels()):\n",
        "        if tick.get_text() == in_tokens[pos_token]:\n",
        "            try:\n",
        "                if axes.get_xticklabels()[ix+1].get_text() == ca:\n",
        "                    if axes.get_xticklabels()[ix-1].get_text() == cp:\n",
        "                        tick.set_color(\"magenta\")\n",
        "                        tick.set_fontsize(10)\n",
        "            except:\n",
        "                tick.set_color(\"magenta\")\n",
        "                tick.set_fontsize(10)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def display_avg_attn_per_layer(in_tokens: list, \n",
        "                               attns: torch.Tensor, \n",
        "                               pos_token: int, \n",
        "                               layer: int) -> None:\n",
        "    \"\"\"Display mean attention weights for token per layer.\"\"\"\n",
        "    layer -= 1\n",
        "    fig, axes = plt.subplots(figsize = (20, 1))\n",
        "    attns = attns.permute(0, 2, 1, 3)\n",
        "    attentions_pos = attns[pos_token]\n",
        "    cp = \" \".join(in_tokens[pos_token-1:pos_token])\n",
        "    ca = \" \".join(in_tokens[pos_token+1:pos_token+2])\n",
        "    context = cp + f\" {in_tokens[pos_token]} \" + ca\n",
        "    avg_attn = attentions_pos[layer].mean(dim=0)\n",
        "    print(f'\\nPer-layer average attention weights')\n",
        "    sns.heatmap(avg_attn.reshape(1, -1), vmin = 0, vmax = 1, xticklabels = in_tokens)\n",
        "    axes.set_title(f'Layer #{layer + 1} ' )\n",
        "    axes.set_ylabel('Heads (avg)')\n",
        "    plt.xticks(rotation=55, fontsize=8)\n",
        "    for ix, tick in enumerate(axes.get_xticklabels()):\n",
        "        if tick.get_text() == in_tokens[pos_token]:\n",
        "            try:\n",
        "                if axes.get_xticklabels()[ix+1].get_text() == ca:\n",
        "                    if axes.get_xticklabels()[ix-1].get_text() == cp:\n",
        "                        tick.set_color(\"magenta\")\n",
        "                        tick.set_fontsize(10)\n",
        "            except:\n",
        "                tick.set_color(\"magenta\")\n",
        "                tick.set_fontsize(10)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT1UXICXw0xV",
        "colab_type": "text"
      },
      "source": [
        "#### Logits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAwbbtiOSTdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_gen(mpath: str='', typ: str='2', dev: str='cuda') -> Tuple:\n",
        "    \"\"\"Get subclassed model, tokenizer from pretrained.\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(mpath)\n",
        "    if typ == \"2\":\n",
        "        model = GPT2DoubleHeadsModel.from_pretrained(mpath, \n",
        "                                                     output_attentions=True)\n",
        "    else:\n",
        "        model = GPT2LMHeadModel.from_pretrained(mpath)\n",
        "    model.eval() # deactivate dropout for reproducibility\n",
        "    model.to(dev)\n",
        "    return tokenizer, model\n",
        "\n",
        "# Define function for loading and processing ROCStories data file for generation prompts.\n",
        "def load_rocstories_dataset(dataset_path: str) -> Tuple[List[str], pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Output story, ending w/ special tokens.\"\"\"\n",
        "    sop = \"_start_\"\n",
        "    eop = \"_delimiter_\"\n",
        "    roc_df = pd.read_csv(dataset_path, sep=',', usecols=[2, 3, 4, 5, 6])\n",
        "    df_endings = roc_df.sentence5\n",
        "    df_stories = roc_df.loc[:, :'sentence4'].copy()\n",
        "    story_bodies = df_stories.sentence1 + \" \" + df_stories.sentence2 + \" \" + df_stories.sentence3 + \" \" + df_stories.sentence4\n",
        "    df_stories.sentence1 = sop + df_stories.sentence1\n",
        "    df_stories.sentence4 = df_stories.sentence4 + eop\n",
        "    return story_bodies, df_stories, df_endings\n",
        "    \n",
        "def model_c(model: str) -> str:\n",
        "    return model\n",
        "\n",
        "def get_endings(mname: str) -> list:\n",
        "    \"\"\"Use generation function to collect stories+endings.\"\"\"\n",
        "    gen_endings = pd.read_csv(f\"endings/{mname}_gen_ends.txt\", sep='\\t').values.tolist()\n",
        "    return gen_endings\n",
        "\n",
        "def load_res(mname: str) -> pd.DataFrame:\n",
        "    \"\"\"Load results as DataFrame.\"\"\"\n",
        "    df = pd.read_csv(f\"stories/seg_results_{mname}.txt\", sep='\\t', error_bad_lines=False)\n",
        "    return df\n",
        "\n",
        "def get_logits(gen: str, cref: str, model, tokenizer, mname: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Get logits for generated endings.\"\"\"\n",
        "    input_ids_gen = torch.tensor(tokenizer.encode(gen, add_special_tokens=False)).unsqueeze(0).to(\"cuda\")  # Batch size 1\n",
        "    input_ids_cref = torch.tensor(tokenizer.encode(cref, add_special_tokens=False)).unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_gen = model(input_ids_gen, lm_labels=input_ids_gen)\n",
        "        _, lm_logits, mc_logits, _, attns = outputs_gen # lm_loss, lm_logits, mc_logits, presents, attns\n",
        "        outputs_cref = model(input_ids_cref, lm_labels=input_ids_cref)\n",
        "    return lm_logits, attns\n",
        "\n",
        "def collect_logits(gen_stories: list, model, tokenizer, mname: str) -> list:\n",
        "    \"\"\"Collect each story's perplexity results for generated, correct endings.\"\"\"\n",
        "    df_endings_list = df_endings.values.tolist()\n",
        "    results = []\n",
        "    for ix, story in enumerate(gen_stories):\n",
        "        story = story.split(\"\\t\")\n",
        "        try:\n",
        "            gen_end = story[1]\n",
        "        except:\n",
        "            gen_end = \"_none_\"\n",
        "        if not mname == 'gpt2':\n",
        "            story_body = \"_start_\" + story[0]\n",
        "            gen = story_body + \"_delimiter_\" + gen_end\n",
        "            cref = story_body + \"_delimiter_\" + df_endings_list[ix]\n",
        "        else:\n",
        "            story_body = story[0]\n",
        "            gen = story_body + gen_end\n",
        "            cref = story_body + df_endings_list[ix]\n",
        "        \n",
        "        lm_logits, attns = get_logits(gen, cref, model, tokenizer, mname)\n",
        "        results.append([lm_logits, story_body, (df_endings_list[ix], gen_end)])\n",
        "    # Return small batch of logits for softmax viz, the rest for attn viz.\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OagNdmAqwxgl",
        "colab_type": "text"
      },
      "source": [
        "#### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WgOslPNwuhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From HuggingFace\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\"), min_tokens_to_keep=1):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size, vocabulary size)\n",
        "            if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "            Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        if min_tokens_to_keep > 1:\n",
        "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
        "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "#\n",
        "    \n",
        "def softmax(temperature):\n",
        "    \"\"\"Softmax w/ temp for logits.\"\"\"\n",
        "    global old_logits\n",
        "    v = len(old_logits)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.title(\"Softmax distribution w/ temperature\")\n",
        "    plt.xlabel(\"Vocabulary\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    probs = F.softmax(old_logits/temperature, dim=-1)\n",
        "    plt.bar(range(v),\n",
        "            probs, \n",
        "            facecolor='cyan', \n",
        "            edgecolor='blue')\n",
        "    sample = torch.multinomial(probs, replacement=True, num_samples=v)\n",
        "    next_token = tokenizer.decode(sample)\n",
        "    plt.xticks(sample, next_token)\n",
        "    \n",
        "def softmaxp(temperature):\n",
        "    \"\"\"Softmax w/ temp for truncated logits.\"\"\"\n",
        "    global p_logits\n",
        "    v = len(p_logits)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.title(f\"Truncated distribution w/ temperature; k: {top_k}, p: {top_p}\")\n",
        "    plt.xlabel(\"Vocabulary\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    probs = F.softmax(p_logits/temperature, dim=-1)\n",
        "    plt.bar(range(v), \n",
        "            probs, \n",
        "            facecolor='magenta', \n",
        "            edgecolor='purple')\n",
        "    sample = torch.multinomial(probs, replacement=True, num_samples=v)\n",
        "    next_token = tokenizer.decode(sample)\n",
        "    plt.xticks(sample, next_token)\n",
        "\n",
        "def get_probs(plogits: torch.Tensor, prompt: str, ending: str, typ: str, tau: float = 1.0) -> list:\n",
        "    \"\"\"WIP: Get probabilities models assigns to each token.\"\"\"\n",
        "    prompt_tokens = tokenizer.tokenize(prompt)\n",
        "    ending = ending\n",
        "    end_tokens = tokenizer.tokenize(ending)\n",
        "    if prompt != \"_start_\":\n",
        "        story_tokens = prompt_tokens + end_tokens\n",
        "    else:\n",
        "        story_tokens = end_tokens\n",
        "        plogits = plogits[:, 1:, 1:]\n",
        "    story_tokens = list(map(lambda w: w.replace(\"Ġ\", \"\"), story_tokens))\n",
        "    ids = torch.tensor(tokenizer.convert_tokens_to_ids(story_tokens)).unsqueeze(0)\n",
        "    probs = F.softmax(plogits[0]/tau, dim=-1)\n",
        "    unmask = torch.zeros(plogits[0].shape)\n",
        "    for i, v in enumerate(ids[0]):\n",
        "        unmask[i, v] = probs[i, v]\n",
        "    unmask = unmask[unmask != 0]\n",
        "    n = plogits[0].shape[0]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(f\"Original probabilities assigned by model to {typ} ending tokens.\")\n",
        "    plt.xlabel(\"Vocabulary\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    #new_probs = []\n",
        "    #for i in range(n):\n",
        "    #    new_probs.append(probs[i][ids[0][i]])\n",
        "    c = 'orange' if typ == 'generated' else 'blue'\n",
        "    plt.plot(unmask, color=c)\n",
        "    plt.xticks(range(len(story_tokens)), story_tokens, rotation=45)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nsNosKa2G6qb"
      },
      "source": [
        "## III. Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUX2gW_EPxj0",
        "colab_type": "text"
      },
      "source": [
        "#### Load model, tokenizer and run on toy examples for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaexEkTZAIJL",
        "colab_type": "text"
      },
      "source": [
        "> Run the following cell to choose model from dropdown:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-DHkeCMipFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_chooser = interactive(model_c, model=[('Base', 'gpt2'),\n",
        "                                            ('Base to Sentiment to SCT', 'b_sentiment_SCT'),\n",
        "                                            ('Base to ConceptNet', 'conceptnet'),\n",
        "                                            ('Base to ROC', 'roc1617'),\n",
        "                                            ('Base to SCT', 'b_SCT'),\n",
        "                                            ('ConceptNet to SCT', 'cn_SCT'),\n",
        "                                            ('ROC to SCT', 'roc1617_SCT'),\n",
        "                                            ('CN to Sentiment to SCT', 'cn_sentiment_SCT'),\n",
        "                                            ('ROC to Sentiment to SCT', 'roc1617_sentiment_SCT')])\n",
        "display(model_chooser)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxbMY3CRAOUx",
        "colab_type": "text"
      },
      "source": [
        "> Once the model is chosen, run the next cell get the model and tokenizer, set parameters and load the stories and endings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHdg9x1Fn_vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model_chooser and model_chooser.result:\n",
        "    mname = model_chooser.result\n",
        "    \n",
        "model_path = f'models/{mname}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cieTffEuJGHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer, model = get_model_gen(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnI8UgAnE0OG",
        "colab_type": "text"
      },
      "source": [
        "### Visualize losses vs. epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgq5RTUKE23W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e1_mc_losses, e2_mc_losses, e3_mc_losses = [], [], []\n",
        "e1_lm_losses, e2_lm_losses, e3_lm_losses = [], [], []\n",
        "models = ['b_SCT', 'cn_SCT', 'roc1617_SCT']\n",
        "models2 = ['b_sentiment_SCT', 'cn_sentiment_SCT', 'roc1617_sentiment_SCT']\n",
        "for mname in models:\n",
        "    with open(f\"models/{mname}/train_results.txt\") as trainres:\n",
        "        lines = trainres.read().splitlines()\n",
        "\n",
        "        e1_mc_losses.append(eval(lines[0][19:]))\n",
        "        e1_lm_losses.append(eval(lines[1][19:]))\n",
        "\n",
        "        e2_mc_losses.append(eval(lines[2][19:]))\n",
        "        e2_lm_losses.append(eval(lines[3][19:]))\n",
        "\n",
        "        e3_mc_losses.append(eval(lines[4][19:]))\n",
        "        e3_lm_losses.append(eval(lines[5][19:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhC9zsl2vIWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reverse labels until MTL script is rerun due to foolish tuple indexing error (corrected).\n",
        "\n",
        "lm_losses = {'e1': e1_mc_losses, 'e2': e2_mc_losses, 'e3': e3_mc_losses}\n",
        "mc_losses = {'e1': e1_lm_losses, 'e2': e2_lm_losses, 'e3': e3_lm_losses}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDqS3isXzRpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 8))\n",
        "fig.tight_layout(pad=4)\n",
        "axs[0].set_title(f\"Multiple Choice (MC) + Language Modeling (LM) fine-tuning losses\\n\")\n",
        "axs[0].grid(True)\n",
        "axs[1].grid(True)\n",
        "\n",
        "for e, c in zip(['e1', 'e2', 'e3'], ['cyan', 'coral', 'lime']):\n",
        "    axs[0].plot([1, 2, 3], mc_losses[e], color=c)\n",
        "    axs[0].set_ylabel(\"MC Loss\")\n",
        "    axs[0].set_ylim(top=1, bottom=0)\n",
        "    axs[0].legend(models)\n",
        "\n",
        "    axs[1].plot([1, 2, 3], lm_losses[e])\n",
        "    axs[1].legend(models)\n",
        "    axs[1].set_xticks([1, 2, 3])\n",
        "    axs[1].set_xlabel(\"Epochs\")\n",
        "    axs[1].set_ylabel(\"LM Loss\")\n",
        "    axs[1].set_ylim(top=8, bottom=2)\n",
        "\n",
        "plt.savefig(f'devoirs/survey/figures/models_losses.png', transparent=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkshGdxFYOR",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8BnLPX3CtTL",
        "colab_type": "text"
      },
      "source": [
        "> Get first story results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsLl9TNIBL7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dpath = \"datasets/roc_1617_test.csv\"\n",
        "story_bodies, df_stories, df_endings = load_rocstories_dataset(dpath)\n",
        "# stories = df_stories.join(df_endings).values.tolist()\n",
        "# gen_endings = get_endings(mname)\n",
        "\n",
        "gen_stories = load_res(mname)\n",
        "stories = gen_stories.Story + \"\\t\" + \" \" + gen_stories.GenEnding\n",
        "\n",
        "story = stories[0]\n",
        "\n",
        "num_logits = 50 # We just want enough to demonstrate temperature. More than 100 or so and the notebook explodes.\n",
        "top_p = 0.9\n",
        "top_k = 20\n",
        "\n",
        "results = collect_logits(gen_stories = [story], \n",
        "                         model = model, \n",
        "                         tokenizer = tokenizer, \n",
        "                         mname = mname)\n",
        "full_logits, prompt, endings = results[0]\n",
        "logits = full_logits[0, 0, :num_logits].cpu()\n",
        "ending = endings[0] # endings: (Correct Ending, Generated Ending)\n",
        "\n",
        "# Filter logits for truncated visualization\n",
        "old_logits = logits.clone() # Avoid zany side-effects from visualization functions.\n",
        "p_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "# Get attention weights, set necessary visualization variables\n",
        "in_tokens, pos_token, attns = get_attns(prompt, \n",
        "                                        ending, \n",
        "                                        model, \n",
        "                                        tokenizer, \n",
        "                                        mname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVFCzDcyoE0y",
        "colab_type": "text"
      },
      "source": [
        "Ending probabilities according to model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyVTp8jEQ-OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_plot(ix: int):\n",
        "    gen_ending = gen_stories.GenEnding[ix]\n",
        "\n",
        "    results = collect_logits(gen_stories = [\"\\t\" + gen_ending], \n",
        "                            model = model, \n",
        "                            tokenizer = tokenizer, \n",
        "                            mname = mname)\n",
        "    full_logits, prompt, endings = results[0]\n",
        "    get_probs(full_logits.cpu(), prompt, gen_ending, typ=\"generated\")\n",
        "\n",
        "    cstory = gen_stories.CorrectEnding[ix]\n",
        "\n",
        "    results = collect_logits(gen_stories = [\"\\t\" + cstory], \n",
        "                            model = model, \n",
        "                            tokenizer = tokenizer, \n",
        "                            mname = mname)\n",
        "    full_logits, prompt, endings = results[0]\n",
        "\n",
        "    in_tokens, pos_token, attns = get_attns(prompt, \n",
        "                                            ending, \n",
        "                                            model, \n",
        "                                            tokenizer, \n",
        "                                            mname)\n",
        "    \n",
        "    get_probs(full_logits.cpu(), prompt, cstory, typ=\"correct\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBEAbx1Qos0C",
        "colab_type": "text"
      },
      "source": [
        "Plot ending probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7mHS14huPMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interactive(run_plot, ix=range(0, len(gen_stories.Story)), continuous=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKVf4LchQKWT",
        "colab_type": "text"
      },
      "source": [
        "### Main visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o99WZ3K5e14y",
        "colab_type": "text"
      },
      "source": [
        "#### Run the next cell for softmax visualization on toy example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LH-XvXoaG6qM",
        "colab": {}
      },
      "source": [
        "w = interactive(softmax, \n",
        "                temperature=FloatSlider(min=0.1, \n",
        "                                        max=10.01, \n",
        "                                        step=0.1, \n",
        "                                        description='Temperature:', \n",
        "                                        value=1.0, \n",
        "                                        continuous_update=False))\n",
        "y = interactive(softmaxp, \n",
        "                temperature=FloatSlider(min=0.1, \n",
        "                                        max=10.01, \n",
        "                                        step=0.1, \n",
        "                                        description='Temperature:', \n",
        "                                        value=1.0, \n",
        "                                        continuous_update=False))\n",
        "\n",
        "w.layout.height = '450px'\n",
        "y.layout.height = '450px'\n",
        "display(w, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt4wTL5meHWT",
        "colab_type": "text"
      },
      "source": [
        "#### Attention visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8_vYNT7elHl",
        "colab_type": "text"
      },
      "source": [
        "##### Run any below to map attention by token position:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXcMvlvEeNPz",
        "colab_type": "text"
      },
      "source": [
        "> All heads (slow):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o3BPUdV7hYMB",
        "colab": {}
      },
      "source": [
        "dph_all = interactive(display_attns, \n",
        "                      {'manual': True,\n",
        "                       'manual_name': 'Display attns (all)'},\n",
        "                        in_tokens=fixed(in_tokens), \n",
        "                        attns=fixed(attns), \n",
        "                        pos_token=IntSlider(min=0, \n",
        "                                            max=len(in_tokens)-1, \n",
        "                                            step=1, \n",
        "                                            description='position:', \n",
        "                                            value=1, \n",
        "                                            continuous_update=False))\n",
        "                                      \n",
        "display(dph_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2fXc4_rD54k",
        "colab_type": "text"
      },
      "source": [
        "> Per head:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YPC5Vl_ZhYL8",
        "colab": {}
      },
      "source": [
        "dph = interactive(display_per_head_attn, \n",
        "                  in_tokens=fixed(in_tokens), \n",
        "                  attns=fixed(attns), \n",
        "                  pos_token=IntSlider(min=0, \n",
        "                                      max=len(in_tokens)-1, \n",
        "                                      step=1, \n",
        "                                      description='position:', \n",
        "                                      value=1, \n",
        "                                      continuous_update=False), \n",
        "                  head=IntSlider(min=1, \n",
        "                                 max=12, \n",
        "                                 step=1, \n",
        "                                 description='head:', \n",
        "                                 value=12, \n",
        "                                 continuous_update=False))\n",
        "dph.layout.height = '650px'\n",
        "display(dph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIRjtxfREAYZ",
        "colab_type": "text"
      },
      "source": [
        "> Per layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5UDyzQv8hYL3",
        "colab": {}
      },
      "source": [
        "dpl = interactive(display_per_layer_attn, \n",
        "                  in_tokens=fixed(in_tokens), \n",
        "                  attns=fixed(attns), \n",
        "                  pos_token=IntSlider(min=0, \n",
        "                                      max=len(in_tokens)-1, \n",
        "                                      step=1, \n",
        "                                      description='position:', \n",
        "                                      value=1, \n",
        "                                      continuous_update=False), \n",
        "                  layer=IntSlider(min=1, \n",
        "                                 max=12, \n",
        "                                 step=1, \n",
        "                                 description='layer:', \n",
        "                                 value=12, \n",
        "                                 continuous_update=False))\n",
        "dpl.layout.height = '650px'\n",
        "display(dpl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCez2Qg8EGqU",
        "colab_type": "text"
      },
      "source": [
        "> Per head, per layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gxqdKA5IhYLv",
        "colab": {}
      },
      "source": [
        "dplh = interactive(display_per_layer_per_head_attn, \n",
        "                  in_tokens=fixed(in_tokens), \n",
        "                  attns=fixed(attns), \n",
        "                  pos_token=IntSlider(min=0, \n",
        "                                      max=len(in_tokens)-1, \n",
        "                                      step=1, \n",
        "                                      description='position:', \n",
        "                                      value=1, \n",
        "                                      continuous_update=False), \n",
        "                  layer=IntSlider(min=1, \n",
        "                                 max=12, \n",
        "                                 step=1, \n",
        "                                 description='layer:', \n",
        "                                 value=12, \n",
        "                                 continuous_update=False),\n",
        "                  head=IntSlider(min=1, \n",
        "                                 max=12, \n",
        "                                 step=1, \n",
        "                                 description='head:', \n",
        "                                 value=12, \n",
        "                                 continuous_update=False))\n",
        "dplh.layout.height = '300px'\n",
        "display(dplh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEsYLq4rEQXB",
        "colab_type": "text"
      },
      "source": [
        "> Averaged over heads:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sAmTBtmahYLq",
        "colab": {}
      },
      "source": [
        "dph_avg = interactive(display_avg_attn, \n",
        "                  in_tokens=fixed(in_tokens), \n",
        "                  attns=fixed(attns), \n",
        "                  pos_token=IntSlider(min=0, \n",
        "                                      max=len(in_tokens)-1, \n",
        "                                      step=1, \n",
        "                                      description='position:', \n",
        "                                      value=1, \n",
        "                                      continuous_update=False))\n",
        "dph_avg.layout.height = '650px'\n",
        "display(dph_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxRsesS_ItCl",
        "colab_type": "text"
      },
      "source": [
        "> Averaged over layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZyK9O8aI0VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dph_avg = interactive(display_avg_attn_l, \n",
        "                  in_tokens=fixed(in_tokens), \n",
        "                  attns=fixed(attns), \n",
        "                  pos_token=IntSlider(min=0, \n",
        "                                      max=len(in_tokens)-1, \n",
        "                                      step=1, \n",
        "                                      description='position:', \n",
        "                                      value=1, \n",
        "                                      continuous_update=False))\n",
        "dph_avg.layout.height = '650px'\n",
        "display(dph_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDYTFSx_ETaB",
        "colab_type": "text"
      },
      "source": [
        "> Averaged over heads, per layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CibVD99KhYLa",
        "colab": {}
      },
      "source": [
        "dph_avg_l = interactive(display_avg_attn_per_layer, \n",
        "                  in_tokens=fixed(in_tokens), \n",
        "                  attns=fixed(attns), \n",
        "                  pos_token=IntSlider(min=0, \n",
        "                                      max=len(in_tokens)-1, \n",
        "                                      step=1, \n",
        "                                      description='position:', \n",
        "                                      value=1, \n",
        "                                      continuous_update=False), \n",
        "                  layer=IntSlider(min=1, \n",
        "                                 max=12, \n",
        "                                 step=1, \n",
        "                                 description='layer:', \n",
        "                                 value=12, \n",
        "                                 continuous_update=False))\n",
        "dph_avg_l.layout.height = '650px'\n",
        "display(dph_avg_l)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}