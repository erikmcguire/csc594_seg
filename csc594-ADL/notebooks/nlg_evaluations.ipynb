{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GH Copy of nlg_evaluations.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L5x0p5cavzMl",
        "OHY4Zkt04M2G",
        "RE7Gl89w3Ods",
        "6HpoNyknFoHZ",
        "dLombbAtxjVX",
        "i4WmAowWVUKa",
        "CVgu8E2_yit4",
        "i8yAf5o1xzCG",
        "_D7ijkvDyBgn",
        "vIh_03R34Vee",
        "nJGVQ1SF01eE",
        "PNlpJUphIv2S",
        "HGTQYGDzABv0"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5x0p5cavzMl",
        "colab_type": "text"
      },
      "source": [
        "# NLG Evaluations\n",
        "\n",
        "Erik McGuire\n",
        "\n",
        "CSC594-810, Winter 19-20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHY4Zkt04M2G",
        "colab_type": "text"
      },
      "source": [
        "### Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SLQfUv0zswd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statistics as st\n",
        "from string import punctuation as punk\n",
        "from ipywidgets import *\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwj-1UF1wB5w",
        "colab_type": "text"
      },
      "source": [
        "* Import [NLG-eval](https://arxiv.org/abs/1706.09799) for suite of automated metrics: BLEU, ROUGE, METEOR, CIDEr, Skip Thought, GloVe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdlyBZZUcRaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Maluuba/nlg-eval.git\n",
        "%cd nlg-eval\n",
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsoW8aEhtPVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My Drive/csc594-ADL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg6k8GuGb8vH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLGEval functions run from here, looking at below setup path as relative data_path.%cd ../drive/'My Drive'/csc594-ADL\n",
        "!nlg-eval --setup \"nlgeval\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn5m8eaMdosr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nlgeval import NLGEval\n",
        "from nlgeval import compute_metrics, compute_individual_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGUKjQ3QE2Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install vaderSentiment\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u5KLaOBdv37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTnQDyXXW66D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy-readability\n",
        "from spacy_readability import Readability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCvW0-DgpB_f",
        "colab_type": "text"
      },
      "source": [
        "#### Imports and installations for entity coreference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yDqaI75FvYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanza==1.0.0\n",
        "import stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oRLVLnXGhUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy, pprint\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7tsgV0RU35_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import client module\n",
        "from stanza.server import CoreNLPClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScLSG6MItRGs",
        "colab": {}
      },
      "source": [
        "%cd drive/My Drive/csc594-ADL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUiL8mhDU5Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"Downloading CoreNLP...\"\n",
        "# !wget \"http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\" -O corenlp.zip\n",
        "!unzip corenlp.zip\n",
        "!mv ./stanford-corenlp-full-2018-10-05 ./corenlp\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = \"./corenlp\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTQF9r6lU2PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(annotators=['coref', 'tokenize','ssplit', 'pos', 'lemma', 'ner'], memory='4G', endpoint='http://localhost:9001')\n",
        "print(client)\n",
        "\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmQ_1isU-96b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "client.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE7Gl89w3Ods",
        "colab_type": "text"
      },
      "source": [
        "### Select model for evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smvsfPsF3NpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_c(model: str) -> list:\n",
        "    \"\"\"Choose model, load story data.\"\"\"\n",
        "    print(f\"Selected model: {model}\")\n",
        "    gold_refs = f'endings/{model}_corr_ends.txt'\n",
        "    story_refs = f'datasets/story_bodies.txt'\n",
        "    hypotheses = f'endings/{model}_gen_ends.txt'\n",
        "    gen_stories = f'seg_results_{model}.txt'\n",
        "    refs = [story_refs]\n",
        "\n",
        "    stories_split = pd.read_csv(story_refs, sep=\",\", header=None).values.tolist()\n",
        "    stories = [\"\".join(story) for story in stories_split]\n",
        "    with open(gold_refs) as gold_file:\n",
        "        corr_ends = gold_file.readlines()\n",
        "        \n",
        "    with open(hypotheses) as endings_file:\n",
        "        gendings = endings_file.readlines()\n",
        "    return stories_split, model, stories, corr_ends, gendings\n",
        "\n",
        "model_chooser = interactive(model_c, model=[('Base', 'gpt2'),\n",
        "                                            ('Base to SCT', 'b_SCT'), \n",
        "                                            ('ConceptNet to SCT', 'cn_SCT_new'), #\n",
        "                                            ('ConceptNet to Sentiment', 'cn_sentiment'), #\n",
        "                                            ('CN to SCT to Sentiment', 'cn_SCT_sentiment'),\n",
        "                                            ('CN to Sentiment to SCT', 'cn_sentiment_SCT'),\n",
        "                                            ('ROC to SCT to Sentiment', 'roc1617_SCT_sentiment'),\n",
        "                                            ('ROC to Sentiment to SCT', 'roc1617_sentiment_SCT'),\n",
        "                                            ('ROC to SCT', 'roc1617_SCT'),\n",
        "                                            ('ROC to Sentiment', 'roc1617_sentiment')])\n",
        "display(model_chooser)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGPZntcKuvlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Correct mishap with newlines being inserted between ending segments, throwing off row count.\n",
        "gen_stories = f'stories/seg_results_cn_SCT_new.txt'\n",
        "gdf = pd.read_csv(gen_stories, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7i3kBINfyrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdf.dropna(axis=0, inplace=True, thresh=2)\n",
        "gdf.fillna(axis=0, inplace=True, value=\"_none_\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEYW_cz4QIrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stories_split, fname, stories, corr_ends, gendings_alt = model_chooser.result \n",
        "\n",
        "gendings = gdf.GenEnding.tolist()\n",
        "for ix, gending in enumerate(gendings):\n",
        "    if gending.strip() == corr_ends[ix].strip() or len(gending.strip()) == 0:\n",
        "        gendings[ix] = '_none_'\n",
        "\n",
        "for i in range(2):\n",
        "    print(\"\\nStory body sample:\\n\", \".\\n \".join(stories[i].split(\". \")), \n",
        "            \"\\n\\nCorrect ending: \", corr_ends[i], \n",
        "          \"\\nGenerated ending: \", gendings[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HpoNyknFoHZ",
        "colab_type": "text"
      },
      "source": [
        "### Entity Coreference\n",
        "* Roemmele \\[[PDF](https://roemmele.github.io/publications/fiction_generation.pdf)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_f4RRNpwgvu",
        "colab_type": "text"
      },
      "source": [
        "* Analyze entity coherence between story prompts, generated endings. \n",
        "* Metric code adapted from Melissa Roemmele \\[[GitHub](https://github.com/roemmele/narrative-prediction/)]\n",
        "* Stanford CoreNLP code adapted from \\[[GitHub](https://github.com/stanfordnlp/stanfordnlp)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLombbAtxjVX",
        "colab_type": "text"
      },
      "source": [
        "#### Definitions adapted from Roemmele"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmCrXtze_aY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_noun_chunk_complexity(gen_seqs):\n",
        "    '''return number and length of noun chunks in each generated sequence'''\n",
        "    gen_seqs = check_seqs_format(gen_seqs)\n",
        "    chunk_lengths = []\n",
        "    n_chunks = []\n",
        "    seq_lengths = []  # also track sequence length for normalized scores\n",
        "    for gen_seqs_ in gen_seqs:\n",
        "        chunk_lengths_ = []\n",
        "        n_chunks_ = []\n",
        "        seq_lengths_ = []\n",
        "        for gen_seq in gen_seqs_:\n",
        "            gen_seq = encoder(gen_seq)\n",
        "            seq_lengths_.append(len(gen_seq))\n",
        "            seq_chunks = [chunk for chunk in gen_seq.noun_chunks]\n",
        "            n = len(seq_chunks)\n",
        "            if n:\n",
        "                mean_chunk_length = np.mean([len(chunk) for chunk in seq_chunks])\n",
        "            else:\n",
        "                mean_chunk_length = 0  # if no chunks in this sequence, set mean length to 0\n",
        "            chunk_lengths_.append(mean_chunk_length)\n",
        "            n_chunks_.append(n)\n",
        "        n_chunks.append(n_chunks_)\n",
        "        chunk_lengths.append(chunk_lengths_)\n",
        "        seq_lengths.append(seq_lengths_)\n",
        "    n_chunks = np.array(n_chunks)\n",
        "    chunk_lengths = np.array(chunk_lengths)\n",
        "    seq_lengths = np.array(seq_lengths)\n",
        "    norm_n_chunks = n_chunks * 1. / seq_lengths\n",
        "    norm_chunk_lengths = chunk_lengths * 1. / seq_lengths\n",
        "    return {'n_chunks': n_chunks, 'chunk_lengths': chunk_lengths, 'norm_n_chunks': norm_n_chunks, 'norm_chunk_lengths': norm_chunk_lengths,\n",
        "            'mean_n_chunks': np.mean(n_chunks), 'mean_chunk_lengths': np.mean(chunk_lengths),\n",
        "            'norm_mean_n_chunks': np.mean(norm_n_chunks), 'norm_mean_chunk_lengths': np.mean(norm_chunk_lengths)}  # [~np.isnan(chunk_lengths)])}\n",
        "\n",
        "def segment(seq, clauses=False):\n",
        "    if clauses:\n",
        "        seq = segment_into_clauses(seq) #segment into clauses rather than just sentences\n",
        "    else:\n",
        "        seq = [sent.string.strip() for sent in encoder(seq).sents]\n",
        "    return seq\n",
        "    \n",
        "def check_seqs_format(seqs):\n",
        "    '''functions below expect generated sequences to be a list of lists, i.e. multiple sequences for each context sequence;\n",
        "    transform to this format if seqs are a flat list'''\n",
        "    assert(type(seqs) in (list, tuple))\n",
        "    if type(seqs[0]) not in (list, tuple):\n",
        "        seqs = [[seq] for seq in seqs]\n",
        "    return seqs\n",
        "    \n",
        "def get_corefs(context_seqs, gen_seqs, verbose=False):\n",
        "    '''return all the entities in each generated sequence that co-ref to an entity in the corresponding context'''\n",
        "    assert(len(context_seqs) == len(gen_seqs))\n",
        "    assert(type(gen_seqs) in (list, tuple) and type(context_seqs) in (list, tuple))\n",
        "\n",
        "    gen_seqs = check_seqs_format(gen_seqs)\n",
        "\n",
        "    corefs = []\n",
        "    for context_seq_idx, (context_seq, gen_seqs_) in enumerate(zip(context_seqs, gen_seqs)):\n",
        "        n_sents_in_context = len(segment(context_seq))\n",
        "        gen_corefs = []\n",
        "        for gen_seq in gen_seqs_:\n",
        "            seq_corefs = []\n",
        "            seq = context_seq + \" \" + gen_seq\n",
        "            parse = client.annotate(seq, properties={'annotators': 'coref', 'outputFormat': 'json'})\n",
        "            if type(parse) is dict:\n",
        "                #sents = parse['sentences']\n",
        "                for coref_ent_idx, coref_ent in parse['corefs'].items():\n",
        "                    mentions = {'rep_mention': None, 'context_mentions': [], 'gen_mentions': []}\n",
        "                    for mention in coref_ent:\n",
        "                        if mention['isRepresentativeMention']:\n",
        "                            mentions['rep_mention'] = (mention['sentNum'], mention['text'])\n",
        "                        if mention['sentNum'] > n_sents_in_context:  # mention is in generated sequence\n",
        "                            mentions['gen_mentions'].append((mention['sentNum'], mention['text']))\n",
        "                        elif mention['sentNum'] <= n_sents_in_context:\n",
        "                            mentions['context_mentions'].append((mention['sentNum'], mention['text']))\n",
        "                    if mentions['context_mentions']:  # only count corefs between context and generated sequence, not corefs only within generated sequence\n",
        "                        seq_corefs.append(mentions)\n",
        "            gen_corefs.append(seq_corefs)\n",
        "        if verbose and context_seq_idx % 500 == 0:\n",
        "            print(\"processed coreferences in\", context_seq_idx, \"sequences...\")\n",
        "        corefs.append(gen_corefs)\n",
        "\n",
        "    return corefs\n",
        "\n",
        "\n",
        "def get_coref_counts(context_seqs, gen_seqs):\n",
        "    '''return 1) the number of entities (noun chunks) in each generated sequence, 2) the number of entities in each generated sequence that co-refer to entities in its context,\n",
        "    and 3) the proportion of entities in each generated sequence that co-refer to entities in the corresponding context'''\n",
        "    assert(len(context_seqs) == len(gen_seqs))\n",
        "    counts = {'corefs': [], 'prev_mention_sents': []}\n",
        "\n",
        "    corefs = get_corefs(context_seqs, gen_seqs)\n",
        "\n",
        "    for gen_corefs in corefs:\n",
        "        gen_coref_counts = []\n",
        "        #gen_ent_counts = []\n",
        "        gen_prev_mention_sents = []\n",
        "        for seq_corefs in gen_corefs:\n",
        "            coref_counts = sum([len(coref['gen_mentions']) for coref in seq_corefs])\n",
        "            gen_coref_counts.append(coref_counts)\n",
        "            prev_mentions = []\n",
        "            for coref in seq_corefs:\n",
        "                # find the sentence position (number) of the most recent previous mention of each coreferring entity;\n",
        "                # if an entity is the first mention in the generated sequence, look for a coreference in the preceding context sequence;\n",
        "                # if none found or the entity is not the first mention, the previous mention position is the number of the generated sentence itself\n",
        "                # coref_prev_mentions = []\n",
        "                for mention_idx, mention in enumerate(coref['gen_mentions']):\n",
        "                    if mention_idx > 0:\n",
        "                        prev_mentions.append(coref['gen_mentions'][mention_idx - 1][0])\n",
        "                    elif not coref['context_mentions']:\n",
        "                        prev_mentions.append(mention[0])\n",
        "                    else:\n",
        "                        prev_mentions.append(coref['context_mentions'][-1][0])\n",
        "            gen_prev_mention_sents.append(prev_mentions)\n",
        "        counts['corefs'].append(gen_coref_counts)\n",
        "        counts['prev_mention_sents'].append(gen_prev_mention_sents)\n",
        "\n",
        "    counts['ents'] = get_noun_chunk_complexity(gen_seqs)['n_chunks']\n",
        "    #counts['ents'] = np.array(counts['ents'])\n",
        "    counts['mean_ents'] = np.mean(counts['ents'])\n",
        "    counts['corefs'] = np.array(counts['corefs'])\n",
        "    counts['ents'] = np.maximum(counts['ents'], counts['corefs'])  # don't let number of entities exceed the number of coreferences\n",
        "    counts['mean_corefs'] = np.mean(counts['corefs'])\n",
        "    counts['res_rates'] = np.nan_to_num(counts['corefs'] * 1. / counts['ents'])\n",
        "    counts['mean_res_rates'] = np.mean(counts['res_rates'])\n",
        "\n",
        "    return counts\n",
        "\n",
        "def run_corefs(gen_seqs: dict, context_seqs, stat_sig):\n",
        "    coref_counts = {'models':{}, 'p-values':{}}\n",
        "    print(\"\\nCOREFERENCE\")\n",
        "    for model in gen_seqs.keys():\n",
        "        coref_counts['models'][model] = get_coref_counts(context_seqs, gen_seqs[model])\n",
        "    corefdf = pd.DataFrame.from_dict(coref_counts['models'], orient='index')\n",
        "    pprint.pprint(pd.DataFrame.from_dict(coref_counts['models'], orient='index')[['mean_ents', 'mean_corefs', 'mean_res_rates']])\n",
        "    if stat_sig:\n",
        "        coref_counts['p-values']['ents'] = eval_all_diffs({model:analysis['ents']\\\n",
        "                                                            for model,analysis\\\n",
        "                                                            in coref_counts['models'].items()})\n",
        "        coref_counts['p-values']['corefs'] = eval_all_diffs({model:analysis['corefs']\\\n",
        "                                                            for model,analysis\\\n",
        "                                                            in coref_counts['models'].items()})\n",
        "        coref_counts['p-values']['res_rates'] = eval_all_diffs({model:analysis['res_rates']\\\n",
        "                                                                for model,analysis\\\n",
        "                                                                in coref_counts['models'].items()})\n",
        "        print(\"\\np-values:\")\n",
        "        pprint.pprint(pd.DataFrame.from_dict(coref_counts['p-values'], orient='index'))\n",
        "    return corefdf.loc[:, 'mean_ents':'mean_res_rates'].drop('res_rates', axis=1)\n",
        "\n",
        "encoder = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4WmAowWVUKa",
        "colab_type": "text"
      },
      "source": [
        "#### Run corefs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO29oGkA0CYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "story_refs = f'datasets/story_bodies.txt'\n",
        "#ending_refs = f'endings/cn_SCT_new_gen_ends.txt'\n",
        "    \n",
        "context_seqs = pd.read_csv(story_refs, header=None, sep='\\t')[0].values.tolist()\n",
        "gen_seqs = {}\n",
        "#gen_seqs[fname] = pd.read_csv(ending_refs, header=None, sep='_nodel_').values.tolist()\n",
        "gen_seqs[fname] = gendings\n",
        "corefdf = run_corefs(gen_seqs, context_seqs, stat_sig=False)\n",
        "corefdf.to_csv(f\"evals/{fname}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVgu8E2_yit4",
        "colab_type": "text"
      },
      "source": [
        "## Readability\n",
        "* [Novikova](https://arxiv.org/pdf/1707.06875.pdf) et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFhN86dxyy4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "read = Readability()\n",
        "nlp.add_pipe(read, last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MS-BXwtyseV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model_chooser.result:\n",
        "    fname = model_chooser.result\n",
        "    right_refs = f'endings/{fname}_corr_ends.txt'\n",
        "    hypothesis = f'endings/{fname}_gen_ends.txt'\n",
        "    refs = [right_refs]\n",
        "    with open(hypothesis) as hypfile:\n",
        "        gen_endings = hypfile.read().splitlines()\n",
        "    with open(right_refs) as corrfile:\n",
        "        corr_endings = corrfile.read().splitlines()\n",
        "    readability = []\n",
        "    for corr, gen in zip(corr_endings, gen_endings):\n",
        "        refdoc = nlp(corr)\n",
        "        gendoc = nlp(gen)\n",
        "        readability.append([refdoc._.flesch_kincaid_reading_ease, gendoc._.flesch_kincaid_reading_ease])\n",
        "readf = pd.DataFrame(readability, columns=['corr_ease', 'gen_ease'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emhnb-nr693F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "readability = []\n",
        "for corr, gen in zip(corr_ends, gendings):\n",
        "    refdoc = nlp(corr)\n",
        "    gendoc = nlp(gen)\n",
        "    readability.append([refdoc._.flesch_kincaid_reading_ease, gendoc._.flesch_kincaid_reading_ease])\n",
        "readf = pd.DataFrame(readability, columns=['corr_ease', 'gen_ease'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy02thizVeiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "readfm = readf.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8yAf5o1xzCG",
        "colab_type": "text"
      },
      "source": [
        "## NLG-Eval\n",
        "\n",
        "* Sharma et al. \\[[PDF](https://arxiv.org/pdf/1706.09799.pdf)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMb4uMROq-lH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"n = NLGEval(metrics_to_omit=['SkipThoughtCS', \n",
        "                             'VectorExtremaCosineSimilarity',\n",
        "                             'EmbeddingAverageCosineSimilairty', \n",
        "                             'GreedyMatchingScore', \n",
        "                             'EmbeddingAverageCosineSimilairty'])\"\"\"\n",
        "\n",
        "n = NLGEval(metrics_to_omit=['Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'CIDEr', 'ROUGE_L', 'METEOR'])\n",
        "\n",
        "#metrics_dict = n.compute_metrics([corr_ends], gendings)\n",
        "\n",
        "metrics_dict = n.compute_metrics([stories], gendings) # Each element of stories is the story prompt (joined as a string) for the corresponding ending in endings."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IspimrVTutpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdf = pd.DataFrame.from_records([metrics_dict])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D7ijkvDyBgn",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Distinct\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljX5iUtInp0y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Li, Jiwei, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. \"[A Diversity-Promoting Objective Function for Neural Conversation Models.](https://www.aclweb.org/anthology/N16-1014/)\" In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 110-119. 2016.\n",
        "\n",
        "---\n",
        "\n",
        "\"We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences.\"\n",
        "\n",
        "\"... *distinct-1* and *distinct-2* are respectively the number of distinct unigrams and bigrams divided by total number of generated words.\" - Interestingly, See et al. \\[[PDF](https://arxiv.org/pdf/1909.10705.pdf), p. 5] describe and [implement](https://github.com/abisee/story-generation-eval/blob/master/metrics.py) it differently from the authors, with division by the total number of _n_-grams. The original authors' [code](https://github.com/YifanZhou95/diversity-promoting-dialogue-system/blob/e4a83359c22299999dab4bc7882c63b1dec51cf3/MMI_antiLM.ipynb) \\[_distinctEval_()] uses tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBRn0y5kq7fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distinct_n(hyp) -> float:\n",
        "    \"\"\"Compute distinct-n for sequence.\"\"\"\n",
        "    df = pd.DataFrame({'Distinct_1': 0,\n",
        "                       'Distinct_2': 0,\n",
        "                       'Distinct_3': 0,\n",
        "                       'Distinct_4': 0}, index=[0])\n",
        "    with open(f\"{hyp}\") as hfile:\n",
        "        lines = hfile.read().splitlines()\n",
        "        d1s = []\n",
        "        d2s = []\n",
        "        d3s = []\n",
        "        d4s = []\n",
        "        for s in lines:\n",
        "            seq = word_tokenize(s)\n",
        "            n = len(seq) or 1\n",
        "            unigrams = set(seq)\n",
        "            bigrams = set(nltk.bigrams(seq)) # distinct bigrams\n",
        "            trigrams = set(nltk.trigrams(seq))\n",
        "            fourgrams = set(nltk.ngrams(seq, 4))\n",
        "            d1s.append(len(unigrams)/n)\n",
        "            d2s.append(len(bigrams)/n)\n",
        "            d3s.append(len(trigrams)/n)\n",
        "            d4s.append(len(fourgrams)/n)\n",
        "    df.Distinct_1 = st.mean(d1s)\n",
        "    df.Distinct_2 = st.mean(d2s)\n",
        "    df.Distinct_3 = st.mean(d3s)\n",
        "    df.Distinct_4 = st.mean(d4s)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQmSPxfqslZA",
        "colab_type": "text"
      },
      "source": [
        "Attempt at reproducing _repetition-4_ metric as described by [Shao](https://www.aclweb.org/anthology/D19-1321.pdf) et al.; [Guan](https://arxiv.org/pdf/2001.05139.pdf) et al.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mxUQYBrBZcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repetition_n(gendings) -> pd.DataFrame:\n",
        "    \"\"\"Compute percentage of generated texts \n",
        "       that repeats at least one 4-gram.\"\"\"\n",
        "    cnt = 0\n",
        "    for s in gendings:\n",
        "        seq = word_tokenize(s)\n",
        "        fourgrams = list(nltk.ngrams(seq, 4))\n",
        "        if fourgrams:\n",
        "            if not len(set(fourgrams)) == len(fourgrams): # If no 4-gram repetition, lengths equal.\n",
        "                cnt += 1 # Else this text repeats at least one 4-gram.\n",
        "    return cnt/len(gendings) # num_repeating_texts/num_texts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9InJdu1J0nA1",
        "colab_type": "text"
      },
      "source": [
        "### Get distinct-1, 2, 3, 4 scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EriwSH4qrANv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddf = distinct_n(f'endings/cn_SCT_new_gen_ends.txt')\n",
        "display(ddf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0qeHm6b58Fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "repetition_n(gendings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIh_03R34Vee",
        "colab_type": "text"
      },
      "source": [
        "### Alternate BLEU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy2aCvPI4SAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.translate.bleu_score.corpus_bleu(stories, endings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJGVQ1SF01eE",
        "colab_type": "text"
      },
      "source": [
        "## Write evaluations to file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB1llSw72gp8",
        "colab_type": "text"
      },
      "source": [
        "Evaluate endings with respect to references, and write endings' scores to evaluations file:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;<small><u>Note</u>: For some cases, temporarily edited *nlgeval*'s \\_init_.py to return individual story results.</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5BT09dEJr0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdf.drop(\"EmbeddingAverageCosineSimilairty\", axis=1).to_csv(f\"evals/{fname}_story_end_sg_evals.txt\", \n",
        "                                                            mode='w', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUnLT2Lx5KaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIn3DSb3ppQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "readf.to_csv(f\"evals/{fname}_readability.txt\", mode='w', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8mRqgTD3C_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddf.to_csv(f\"evals/cn_SCT_new_distinct_evals.txt\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkGkx-JjrBX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_dist_nlg(fname: str, metrics: dict = None, ddf: dict = None, \n",
        "                   glove=False, skip=False, overlap=False) -> None:\n",
        "    \"\"\"Write BLEU-1, 2, METEOR, ROUGE-L, CIDEr, Distinct-1, 2, 3, 4, embedding scores to file.\"\"\"\n",
        "    if not ddf.empty:\n",
        "        df = mdf.join(ddf)\n",
        "    else:\n",
        "        df = mdf\n",
        "    try:\n",
        "        df = df.drop(columns=['EmbeddingAverageCosineSimilairty'])\n",
        "    except:\n",
        "        pass\n",
        "    if overlap and not glove and not skip:\n",
        "        df.to_csv(f\"evals/{fname}_all_evals.txt\", index=False)\n",
        "    if glove and not overlap:\n",
        "        df.loc[:, 'EmbeddingAverageCosineSimilarity': ].to_csv(f\"evals/{fname}_sg_glove_evals.txt\", index=False)\n",
        "    if skip and not overlap:\n",
        "        pd.DataFrame(data=[df.SkipThoughtCS.values], \n",
        "                     columns=['SkipThoughtCS']).to_csv(f\"evals/{fname}_sg_skip_evals.txt\", \n",
        "                                                       index=False)\n",
        "    if skip and overlap and glove:\n",
        "        df.to_csv(f\"evals/{fname}_o_skip_glove_evals.txt\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EowMTWWrCZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_dist_nlg(fname=fname, metrics=mdf, \n",
        "               glove=False, skip=False, overlap=True,\n",
        "               ddf=ddf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNlpJUphIv2S",
        "colab_type": "text"
      },
      "source": [
        "#### Originally to create data in seg_evaluations notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ityu0l01Itu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_overlap_all(models: list = []) -> list:\n",
        "    models = ['gpt2',\n",
        "              'b_SCT', \n",
        "              'cn_SCT', \n",
        "              'cn_SCT_sentiment', \n",
        "              'cn_sentiment_SCT', \n",
        "              'roc1617_SCT_sentiment', \n",
        "              'roc1617_sentiment_SCT', \n",
        "              'roc1617_SCT']\n",
        "    model_keys = ['Base',\n",
        "                  'Base_SCT',\n",
        "                  'ConceptNet_SCT',\n",
        "                  'CN_SCT_Sentiment',\n",
        "                  'CN_Sentiment_SCT',\n",
        "                  'ROC_SCT_Sentiment',\n",
        "                  'ROC_Sentiment_SCT',\n",
        "                  'ROC_SCT']\n",
        "    df_list = []\n",
        "    try:\n",
        "        corpus_all_df = pd.read_csv(f'evals/models_overlap.txt', sep=',')\n",
        "        if not corpus_all_df.columns.values[0] == \"Models\":\n",
        "            corpus_all_df.columns.values[0] = \"Models\"\n",
        "            corpus_all_df.columns.values[1] = \"drop\"\n",
        "            corpus_all_df = corpus_all_df.drop(\"drop\", axis=1)\n",
        "            corpus_all_df.to_csv(f'evals/models_overlap.txt', sep=',', index=False)\n",
        "    except:\n",
        "        for mod in models:\n",
        "            corpus_df = pd.read_csv(f\"evals/{mod}_corpus_all.txt\", sep=',')\n",
        "            df_list.append(corpus_df)\n",
        "        corpus_all_df = pd.concat(df_list, axis=0, keys=model_keys)\n",
        "        corpus_all_df.to_csv(f'evals/models_overlap.txt', sep=',', index=True)\n",
        "    corpus_all_df = corpus_all_df.style.hide_index().\\\n",
        "                            apply(bold_max).\\\n",
        "                            apply(ital_min)\n",
        "    if corpus_all_df.columns.values[0] == \"Models\":\n",
        "        return corpus_all_df\n",
        "    else:\n",
        "        return \"Built table. Run cell again.\"\n",
        "\n",
        "def display_distinct_all(models: list = []) -> list:\n",
        "    models = ['gpt2',\n",
        "              'b_SCT', \n",
        "              'cn_SCT', \n",
        "              'cn_SCT_sentiment', \n",
        "              'cn_sentiment_SCT', \n",
        "              'roc1617_SCT_sentiment', \n",
        "              'roc1617_sentiment_SCT', \n",
        "              'roc1617_SCT']\n",
        "    model_keys = ['Base',\n",
        "                  'Base_SCT',\n",
        "                  'ConceptNet_SCT',\n",
        "                  'CN_SCT_Sentiment',\n",
        "                  'CN_Sentiment_SCT',\n",
        "                  'ROC_SCT_Sentiment',\n",
        "                  'ROC_Sentiment_SCT',\n",
        "                  'ROC_SCT']\n",
        "    df_list = []\n",
        "    try:\n",
        "        distinct_all_df = pd.read_csv(f'evals/models_distinct.txt', sep=',')\n",
        "        if not distinct_all_df.columns.values[0] == \"Models\":\n",
        "            distinct_all_df.columns.values[0] = \"Models\"\n",
        "            distinct_all_df.columns.values[1] = \"drop\"\n",
        "            distinct_all_df = distinct_all_df.drop(\"drop\", axis=1)\n",
        "            distinct_all_df.to_csv(f'evals/models_distinct.txt', sep=',', index=False)\n",
        "    except:\n",
        "        for mod in models:\n",
        "            distinct_df = pd.read_csv(f\"evals/{mod}_distinct_evals.txt\", sep=',')\n",
        "            df_list.append(distinct_df)\n",
        "        distinct_all_df = pd.concat(df_list, axis=0, keys=model_keys)\n",
        "        distinct_all_df.to_csv(f'evals/models_distinct.txt', sep=',', index=True)\n",
        "    distinct_all_df = distinct_all_df.style.hide_index().\\\n",
        "                            apply(bold_max).\\\n",
        "                            apply(ital_min)\n",
        "    if distinct_all_df.columns.values[0] == \"Models\":\n",
        "        return distinct_all_df\n",
        "    else:\n",
        "        return \"Built table. Run cell again.\"\n",
        "\n",
        "\n",
        "def display_ppl_all(models: list = []) -> list:\n",
        "    models = ['gpt2',\n",
        "              'b_SCT', \n",
        "              'cn_SCT', \n",
        "              'cn_SCT_sentiment', \n",
        "              'cn_sentiment_SCT', \n",
        "              'roc1617_SCT_sentiment', \n",
        "              'roc1617_sentiment_SCT', \n",
        "              'roc1617_SCT']\n",
        "    model_keys = ['Base',\n",
        "                  'Base_SCT',\n",
        "                  'ConceptNet_SCT',\n",
        "                  'CN_SCT_Sentiment',\n",
        "                  'CN_Sentiment_SCT',\n",
        "                  'ROC_SCT_Sentiment',\n",
        "                  'ROC_Sentiment_SCT',\n",
        "                  'ROC_SCT']\n",
        "    df_list = []\n",
        "    try:\n",
        "        all_ppl_df = pd.read_csv(f'evals/models_ppl.txt', sep=',')\n",
        "        try:\n",
        "            if not all_ppl_df.columns.values[0] == \"Models\":\n",
        "                all_ppl_df.columns.values[0] = \"Models\"\n",
        "                all_ppl_df.columns.values[1] = \"Type\"\n",
        "                all_ppl_df.columns.values[2] = \"PPL\"\n",
        "                all_ppl_df = all_ppl_df.drop(\"Story\", axis=1)\n",
        "                all_ppl_df = all_ppl_df.drop(\"CorrectEnding\", axis=1)\n",
        "                all_ppl_df = all_ppl_df.drop(\"GenEnding\", axis=1)\n",
        "                all_ppl_df.to_csv(f'evals/models_ppl.txt', sep=',', index=False)\n",
        "        except:\n",
        "            pass\n",
        "    except:\n",
        "        for mod in models:\n",
        "            try:\n",
        "                all_ppl = pd.read_csv(f\"evals/{mod}_ppl.csv\", sep=',')\n",
        "                df_list.append(all_ppl.mean(axis=0))\n",
        "            except:\n",
        "                continue\n",
        "        all_ppl_df = pd.concat(df_list, axis=0, keys=model_keys)\n",
        "        all_ppl_df.to_csv(f'evals/models_ppl.txt', sep=',', index=True)\n",
        "\n",
        "    if all_ppl_df.columns.values[0] == \"Models\":\n",
        "        return all_ppl_df\n",
        "    else:\n",
        "        return \"Built table. Run cell again.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGTQYGDzABv0",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07kB1cXJAJHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "analyser = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPt65piOAKeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentiment(p: float) -> None:\n",
        "    \"\"\"Label compound scores heuristically.\"\"\"\n",
        "    if p >= 0.05:\n",
        "        result = 'POS'\n",
        "    elif p <= -0.05:\n",
        "        result = 'NEG'\n",
        "    else: # -0.05 < p < 0.05\n",
        "        result = 'NEU'\n",
        "    return result\n",
        "\n",
        "def get_avg_senti_label(story: list) -> float:\n",
        "    \"\"\"Compute average sentiment; via VADER paragraph demo.\"\"\"\n",
        "    story_sentiments = 0.0\n",
        "    for sentence in story: # Get avg story/context sentiment.\n",
        "        valences = analyser.polarity_scores(sentence)['compound']\n",
        "        story_sentiments += valences\n",
        "    return get_sentiment(round(story_sentiments / len(story), 4))\n",
        "\n",
        "def get_matches(prompts, endings):\n",
        "    \"\"\"Get ending distance to avg story sentiment.\"\"\"\n",
        "    matches = []\n",
        "    story_scores = [get_avg_senti_label(story) for story in prompts] # Label avg sentences' compound scores in story.\n",
        "    ending_scores = [get_sentiment(analyser.polarity_scores(end)['compound']) for end in endings]\n",
        "    for story_score, end_score in zip(story_scores, ending_scores):\n",
        "        if story_score == end_score:\n",
        "            matches.append(end_score)\n",
        "        else:\n",
        "            matches.append('None')    \n",
        "    return matches, story_scores, ending_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xU8U2wIFfBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_matches, story_scores, ending_scores = get_matches(stories_split, gendings)\n",
        "gen_p = (gen_matches.count('POS') + gen_matches.count('NEG'))/len(gen_matches)\n",
        "#corr_matches, story_scores, ending_scores = get_matches(stories_split, corr_ends)\n",
        "#corr_p = (corr_matches.count('POS') + corr_matches.count('NEG'))/len(corr_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk5ILeTVGqa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {'Model': [f'{fname}'], 'Matches': [gen_p]}\n",
        "df = pd.DataFrame.from_dict(d)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIaynMRsHiAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(f'evals/{fname}_sent_matches.txt', index=False, sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHrSr_DIHkap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models_sent_df = pd.read_csv('evals/models_sent_matches.txt', sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L1nHCxFKGum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(models_sent_df.to_latex(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}