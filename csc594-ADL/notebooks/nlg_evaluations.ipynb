{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlg_evaluations.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OHY4Zkt04M2G",
        "RE7Gl89w3Ods",
        "6HpoNyknFoHZ",
        "dLombbAtxjVX",
        "i4WmAowWVUKa",
        "CVgu8E2_yit4",
        "i8yAf5o1xzCG",
        "_D7ijkvDyBgn"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "290be952c87647419a5fb39c96ef84d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ad3ad5a3092c4765861d76ce6c6502dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1eb28616b065452aae5c3060fd342bba",
              "IPY_MODEL_028828c184da40939333f4301ebc6cca"
            ]
          }
        },
        "ad3ad5a3092c4765861d76ce6c6502dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1eb28616b065452aae5c3060fd342bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "Base",
              "Base to SCT",
              "ConceptNet to SCT",
              "ConceptNet to Sentiment",
              "CN to SCT to Sentiment",
              "CN to Sentiment to SCT",
              "ROC to SCT to Sentiment",
              "ROC to Sentiment to SCT",
              "ROC to SCT",
              "ROC to Sentiment"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_442d8084c507486da5f38a9c36e99cf2",
            "_dom_classes": [],
            "description": "model",
            "_model_name": "DropdownModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_773bf4c4b3d44f7881a7721d4b81daa1"
          }
        },
        "028828c184da40939333f4301ebc6cca": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "stream",
                "metadata": {
                  "tags": []
                },
                "text": "Selected model: gpt2\n",
                "stream": "stdout"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_3dbcd117cfb94de8865dbaf209a45d18",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "442d8084c507486da5f38a9c36e99cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "773bf4c4b3d44f7881a7721d4b81daa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5x0p5cavzMl",
        "colab_type": "text"
      },
      "source": [
        "# NLG Evaluations\n",
        "\n",
        "Erik McGuire\n",
        "\n",
        "CSC594-810, Winter 19-20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHY4Zkt04M2G",
        "colab_type": "text"
      },
      "source": [
        "### Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwj-1UF1wB5w",
        "colab_type": "text"
      },
      "source": [
        "* Import [NLG-eval](https://arxiv.org/abs/1706.09799) for suite of automated metrics: BLEU, ROUGE, METEOR, CIDEr, Skip Thought, GloVe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htIMSYfFh4wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install git+https://github.com/Maluuba/nlg-eval.git@master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT_-tfJPUz20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Maluuba/nlg-eval.git\n",
        "%cd nlg-eval\n",
        "!pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg4H-8Zc3-AN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy-readability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVV4NFXWzwMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nlg-eval --setup\n",
        "# !nlg-eval --setup /root/.cache/nlgeval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1y5k4cKVj4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nlgeval import NLGEval\n",
        "from nlgeval import compute_metrics, compute_individual_metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SLQfUv0zswd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statistics as st\n",
        "from string import punctuation as punk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from ipywidgets import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl1RPFfFkXHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWJXETTwT14a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanfordnlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70pOhK_h1cZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ../drive/My Drive/csc594-ADL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oRLVLnXGhUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy, pprint\n",
        "from spacy_readability import Readability\n",
        "# !python -m spacy download en_core_web_md\n",
        "import en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTQF9r6lU2PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "client = CoreNLPClient(annotators=['ner'], memory='4G', endpoint='http://localhost:9001')\n",
        "print(client)\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7tsgV0RU35_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import client module\n",
        "from stanfordnlp.server import CoreNLPClient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUiL8mhDU5Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the Stanford CoreNLP Java library and unzip it to a ./corenlp folder\n",
        "!echo \"Downloading CoreNLP...\"\n",
        "!wget \"http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\" -O corenlp.zip\n",
        "!unzip corenlp.zip\n",
        "!mv ./stanford-corenlp-full-2018-10-05 ./corenlp\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = \"./corenlp\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE7Gl89w3Ods",
        "colab_type": "text"
      },
      "source": [
        "### Select model for evaluations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smvsfPsF3NpD",
        "colab_type": "code",
        "outputId": "484110be-2c97-4ad2-f3c4-e3efa433d4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "290be952c87647419a5fb39c96ef84d2",
            "ad3ad5a3092c4765861d76ce6c6502dd",
            "1eb28616b065452aae5c3060fd342bba",
            "028828c184da40939333f4301ebc6cca",
            "442d8084c507486da5f38a9c36e99cf2",
            "773bf4c4b3d44f7881a7721d4b81daa1"
          ]
        }
      },
      "source": [
        "def model_c(model: str) -> str:\n",
        "    print(f\"Selected model: {model}\")\n",
        "    return model\n",
        "\n",
        "model_chooser = interactive(model_c, model=[('Base', 'gpt2'),\n",
        "                                            ('Base to SCT', 'b_SCT'), \n",
        "                                            ('ConceptNet to SCT', 'cn_SCT'), #\n",
        "                                            ('ConceptNet to Sentiment', 'cn_sentiment'), #\n",
        "                                            ('CN to SCT to Sentiment', 'cn_SCT_sentiment'),\n",
        "                                            ('CN to Sentiment to SCT', 'cn_sentiment_SCT'),\n",
        "                                            ('ROC to SCT to Sentiment', 'roc1617_SCT_sentiment'),\n",
        "                                            ('ROC to Sentiment to SCT', 'roc1617_sentiment_SCT'),\n",
        "                                            ('ROC to SCT', 'roc1617_SCT'),\n",
        "                                            ('ROC to Sentiment', 'roc1617_sentiment')])\n",
        "display(model_chooser)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "290be952c87647419a5fb39c96ef84d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='model', options=(('Base', 'gpt2'), ('Base to SCT', 'b_SCT'), ('Con…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSWmPoxP3eMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model_chooser.result:\n",
        "    fname = model_chooser.result\n",
        "    ending_refs = f'endings/{fname}_corr_ends.txt'\n",
        "    story_refs = f'datasets/story_bodies.txt'\n",
        "    hypotheses = f'endings/{fname}_gen_ends.txt'\n",
        "    refs = [story_refs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HpoNyknFoHZ",
        "colab_type": "text"
      },
      "source": [
        "### Entity Coreference\n",
        "* Roemmele \\[[PDF](https://roemmele.github.io/publications/fiction_generation.pdf)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_f4RRNpwgvu",
        "colab_type": "text"
      },
      "source": [
        "* Analyze entity coherence between story prompts, generated endings. \n",
        "* Metric code adapted from Melissa Roemmele \\[[GitHub](https://github.com/roemmele/narrative-prediction/)]\n",
        "* Stanford CoreNLP code adapted from \\[[GitHub](https://github.com/stanfordnlp/stanfordnlp)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLombbAtxjVX",
        "colab_type": "text"
      },
      "source": [
        "#### Definitions adapted from Roemmele:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmCrXtze_aY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_noun_chunk_complexity(gen_seqs):\n",
        "    '''return number and length of noun chunks in each generated sequence'''\n",
        "    gen_seqs = check_seqs_format(gen_seqs)\n",
        "    chunk_lengths = []\n",
        "    n_chunks = []\n",
        "    seq_lengths = []  # also track sequence length for normalized scores\n",
        "    for gen_seqs_ in gen_seqs:\n",
        "        chunk_lengths_ = []\n",
        "        n_chunks_ = []\n",
        "        seq_lengths_ = []\n",
        "        for gen_seq in gen_seqs_:\n",
        "            gen_seq = encoder(gen_seq)\n",
        "            seq_lengths_.append(len(gen_seq))\n",
        "            seq_chunks = [chunk for chunk in gen_seq.noun_chunks]\n",
        "            n = len(seq_chunks)\n",
        "            if n:\n",
        "                mean_chunk_length = np.mean([len(chunk) for chunk in seq_chunks])\n",
        "            else:\n",
        "                mean_chunk_length = 0  # if no chunks in this sequence, set mean length to 0\n",
        "            chunk_lengths_.append(mean_chunk_length)\n",
        "            n_chunks_.append(n)\n",
        "        n_chunks.append(n_chunks_)\n",
        "        chunk_lengths.append(chunk_lengths_)\n",
        "        seq_lengths.append(seq_lengths_)\n",
        "    n_chunks = np.array(n_chunks)\n",
        "    chunk_lengths = np.array(chunk_lengths)\n",
        "    seq_lengths = np.array(seq_lengths)\n",
        "    norm_n_chunks = n_chunks * 1. / seq_lengths\n",
        "    norm_chunk_lengths = chunk_lengths * 1. / seq_lengths\n",
        "    return {'n_chunks': n_chunks, 'chunk_lengths': chunk_lengths, 'norm_n_chunks': norm_n_chunks, 'norm_chunk_lengths': norm_chunk_lengths,\n",
        "            'mean_n_chunks': np.mean(n_chunks), 'mean_chunk_lengths': np.mean(chunk_lengths),\n",
        "            'norm_mean_n_chunks': np.mean(norm_n_chunks), 'norm_mean_chunk_lengths': np.mean(norm_chunk_lengths)}  # [~np.isnan(chunk_lengths)])}\n",
        "\n",
        "def segment(seq, clauses=False):\n",
        "    if clauses:\n",
        "        seq = segment_into_clauses(seq) #segment into clauses rather than just sentences\n",
        "    else:\n",
        "        seq = [sent.string.strip() for sent in encoder(seq).sents]\n",
        "    return seq\n",
        "    \n",
        "def check_seqs_format(seqs):\n",
        "    '''functions below expect generated sequences to be a list of lists, i.e. multiple sequences for each context sequence;\n",
        "    transform to this format if seqs are a flat list'''\n",
        "    assert(type(seqs) in (list, tuple))\n",
        "    if type(seqs[0]) not in (list, tuple):\n",
        "        seqs = [[seq] for seq in seqs]\n",
        "    return seqs\n",
        "    \n",
        "def get_corefs(context_seqs, gen_seqs, verbose=False):\n",
        "    '''return all the entities in each generated sequence that co-ref to an entity in the corresponding context'''\n",
        "    assert(len(context_seqs) == len(gen_seqs))\n",
        "    assert(type(gen_seqs) in (list, tuple) and type(context_seqs) in (list, tuple))\n",
        "\n",
        "    gen_seqs = check_seqs_format(gen_seqs)\n",
        "\n",
        "    corefs = []\n",
        "    for context_seq_idx, (context_seq, gen_seqs_) in enumerate(zip(context_seqs, gen_seqs)):\n",
        "        n_sents_in_context = len(segment(context_seq))\n",
        "        gen_corefs = []\n",
        "        for gen_seq in gen_seqs_:\n",
        "            seq_corefs = []\n",
        "            seq = context_seq + \" \" + gen_seq\n",
        "            parse = client.annotate(seq, properties={'annotators': 'coref', 'outputFormat': 'json'})\n",
        "            if type(parse) is dict:\n",
        "                #sents = parse['sentences']\n",
        "                for coref_ent_idx, coref_ent in parse['corefs'].items():\n",
        "                    mentions = {'rep_mention': None, 'context_mentions': [], 'gen_mentions': []}\n",
        "                    for mention in coref_ent:\n",
        "                        if mention['isRepresentativeMention']:\n",
        "                            mentions['rep_mention'] = (mention['sentNum'], mention['text'])\n",
        "                        if mention['sentNum'] > n_sents_in_context:  # mention is in generated sequence\n",
        "                            mentions['gen_mentions'].append((mention['sentNum'], mention['text']))\n",
        "                        elif mention['sentNum'] <= n_sents_in_context:\n",
        "                            mentions['context_mentions'].append((mention['sentNum'], mention['text']))\n",
        "                    if mentions['context_mentions']:  # only count corefs between context and generated sequence, not corefs only within generated sequence\n",
        "                        seq_corefs.append(mentions)\n",
        "            gen_corefs.append(seq_corefs)\n",
        "        if verbose and context_seq_idx % 500 == 0:\n",
        "            print(\"processed coreferences in\", context_seq_idx, \"sequences...\")\n",
        "        corefs.append(gen_corefs)\n",
        "\n",
        "    return corefs\n",
        "\n",
        "\n",
        "def get_coref_counts(context_seqs, gen_seqs):\n",
        "    '''return 1) the number of entities (noun chunks) in each generated sequence, 2) the number of entities in each generated sequence that co-refer to entities in its context,\n",
        "    and 3) the proportion of entities in each generated sequence that co-refer to entities in the corresponding context'''\n",
        "    assert(len(context_seqs) == len(gen_seqs))\n",
        "    counts = {'corefs': [], 'prev_mention_sents': []}\n",
        "\n",
        "    corefs = get_corefs(context_seqs, gen_seqs)\n",
        "\n",
        "    for gen_corefs in corefs:\n",
        "        gen_coref_counts = []\n",
        "        #gen_ent_counts = []\n",
        "        gen_prev_mention_sents = []\n",
        "        for seq_corefs in gen_corefs:\n",
        "            coref_counts = sum([len(coref['gen_mentions']) for coref in seq_corefs])\n",
        "            gen_coref_counts.append(coref_counts)\n",
        "            prev_mentions = []\n",
        "            for coref in seq_corefs:\n",
        "                # find the sentence position (number) of the most recent previous mention of each coreferring entity;\n",
        "                # if an entity is the first mention in the generated sequence, look for a coreference in the preceding context sequence;\n",
        "                # if none found or the entity is not the first mention, the previous mention position is the number of the generated sentence itself\n",
        "                # coref_prev_mentions = []\n",
        "                for mention_idx, mention in enumerate(coref['gen_mentions']):\n",
        "                    if mention_idx > 0:\n",
        "                        prev_mentions.append(coref['gen_mentions'][mention_idx - 1][0])\n",
        "                    elif not coref['context_mentions']:\n",
        "                        prev_mentions.append(mention[0])\n",
        "                    else:\n",
        "                        prev_mentions.append(coref['context_mentions'][-1][0])\n",
        "            gen_prev_mention_sents.append(prev_mentions)\n",
        "        counts['corefs'].append(gen_coref_counts)\n",
        "        counts['prev_mention_sents'].append(gen_prev_mention_sents)\n",
        "\n",
        "    counts['ents'] = get_noun_chunk_complexity(gen_seqs)['n_chunks']\n",
        "    #counts['ents'] = np.array(counts['ents'])\n",
        "    counts['mean_ents'] = np.mean(counts['ents'])\n",
        "    counts['corefs'] = np.array(counts['corefs'])\n",
        "    counts['ents'] = np.maximum(counts['ents'], counts['corefs'])  # don't let number of entities exceed the number of coreferences\n",
        "    counts['mean_corefs'] = np.mean(counts['corefs'])\n",
        "    counts['res_rates'] = np.nan_to_num(counts['corefs'] * 1. / counts['ents'])\n",
        "    counts['mean_res_rates'] = np.mean(counts['res_rates'])\n",
        "\n",
        "    return counts\n",
        "\n",
        "def run_corefs(gen_seqs: dict, context_seqs, stat_sig):\n",
        "    coref_counts = {'models':{}, 'p-values':{}}\n",
        "    print(\"\\nCOREFERENCE\")\n",
        "    for model in gen_seqs.keys():\n",
        "        coref_counts['models'][model] = get_coref_counts(context_seqs, gen_seqs[model])\n",
        "    corefdf = pd.DataFrame.from_dict(coref_counts['models'], orient='index')\n",
        "    pprint.pprint(pd.DataFrame.from_dict(coref_counts['models'], orient='index')[['mean_ents', 'mean_corefs', 'mean_res_rates']])\n",
        "    if stat_sig:\n",
        "        coref_counts['p-values']['ents'] = eval_all_diffs({model:analysis['ents']\\\n",
        "                                                            for model,analysis\\\n",
        "                                                            in coref_counts['models'].items()})\n",
        "        coref_counts['p-values']['corefs'] = eval_all_diffs({model:analysis['corefs']\\\n",
        "                                                            for model,analysis\\\n",
        "                                                            in coref_counts['models'].items()})\n",
        "        coref_counts['p-values']['res_rates'] = eval_all_diffs({model:analysis['res_rates']\\\n",
        "                                                                for model,analysis\\\n",
        "                                                                in coref_counts['models'].items()})\n",
        "        print(\"\\np-values:\")\n",
        "        pprint.pprint(pd.DataFrame.from_dict(coref_counts['p-values'], orient='index'))\n",
        "    return corefdf.loc[:, 'mean_ents':'mean_res_rates'].drop('res_rates', axis=1)\n",
        "\n",
        "encoder = en_core_web_sm.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4WmAowWVUKa",
        "colab_type": "text"
      },
      "source": [
        "#### Run corefs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO29oGkA0CYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_seqs = pd.read_csv(story_refs, header=None, sep='\\t')[0].values.tolist()\n",
        "gen_seqs = {}\n",
        "gen_seqs[fname] = pd.read_csv(hypotheses, header=None, sep='_nodel_').values.tolist()\n",
        "corefdf = run_corefs(gen_seqs, context_seqs, stat_sig=False)\n",
        "corefdf.to_csv(f\"evals/{fname}_corefs.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVgu8E2_yit4",
        "colab_type": "text"
      },
      "source": [
        "## Readability\n",
        "* [Novikova](https://arxiv.org/pdf/1707.06875.pdf) et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFhN86dxyy4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "read = Readability()\n",
        "nlp.add_pipe(read, last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MS-BXwtyseV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model_chooser.result:\n",
        "    fname = model_chooser.result\n",
        "    right_refs = f'endings/{fname}_corr_ends.txt'\n",
        "    hypothesis = f'endings/{fname}_gen_ends.txt'\n",
        "    refs = [right_refs]\n",
        "    with open(hypothesis) as hypfile:\n",
        "        gen_endings = hypfile.read().splitlines()\n",
        "    with open(right_refs) as corrfile:\n",
        "        corr_endings = corrfile.read().splitlines()\n",
        "    readability = []\n",
        "    for corr, gen in zip(corr_endings, gen_endings):\n",
        "        refdoc = nlp(corr)\n",
        "        gendoc = nlp(gen)\n",
        "        readability.append([refdoc._.flesch_kincaid_reading_ease, gendoc._.flesch_kincaid_reading_ease])\n",
        "readf = pd.DataFrame(readability, columns=['corr_ease', 'gen_ease'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy02thizVeiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "readf.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8yAf5o1xzCG",
        "colab_type": "text"
      },
      "source": [
        "## NLG-Eval\n",
        "\n",
        "* Sharma et al. \\[[PDF](https://arxiv.org/pdf/1706.09799.pdf)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMb4uMROq-lH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = NLGEval(metrics_to_omit=['SkipThoughtCS', 'VectorExtremaCosineSimilarity',\n",
        "                             'EmbeddingAverageCosineSimilairty', \n",
        "                             'GreedyMatchingScore',\n",
        "                             'EmbeddingAverageCosineSimilarity'])\n",
        "\n",
        "# or omit 'Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'CIDEr', 'ROUGE_L', 'METEOR'\n",
        "\n",
        "with open(story_refs) as stories_file:\n",
        "    stories = stories_file.readlines()[:100]\n",
        "    \n",
        "with open(hypotheses) as endings_file:\n",
        "    endings = endings_file.readlines()[:100]\n",
        "\n",
        "#mds = []\n",
        "#md = {'SkipThoughtCS': [],\n",
        "     'VectorExtremaCosineSimilarity': []}\n",
        "\"\"\"for story, ending in zip(stories, endings):\n",
        "    metrics_dict = n.compute_individual_metrics([story], ending)\n",
        "    for k, v in metrics_dict.items():\n",
        "        md[k].append(v)\"\"\"\n",
        "metrics_dict = n.compute_metrics([stories], endings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IspimrVTutpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdf = pd.DataFrame.from_records([metrics_dict])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFUkJ1C59KpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdf.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5BT09dEJr0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdf_skip = mdf.SkipThoughtCS\n",
        "mdf_glove = mdf.drop('SkipThoughtCS', axis=1)\n",
        "#mdf.to_csv(f\"evals/{fname}_skip_glove_evals.txt\", mode='w', index=False)\n",
        "#mdf_glove.to_csv(f\"evals/{fname}_sg_glove_evals.txt\", mode='w', index=False)\n",
        "#mdf_skip.to_csv(f\"evals/{fname}_sg_skip_evals.txt\", mode='w', index=False)\n",
        "#readf.to_csv(f\"evals/{fname}_readability.txt\", mode='w', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D7ijkvDyBgn",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Distinct\n",
        "\n",
        "---\n",
        "\n",
        "Li, Jiwei, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. \"[A Diversity-Promoting Objective Function for Neural Conversation Models.](https://www.aclweb.org/anthology/N16-1014/)\" In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 110-119. 2016.\n",
        "\n",
        "---\n",
        "\n",
        "\"We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences.\"\n",
        "\n",
        "\"... *distinct-1* and *distinct-2* are respectively the number of distinct unigrams and bigrams divided by total number of generated words.\" - Interestingly, See et al. \\[[PDF](https://arxiv.org/pdf/1909.10705.pdf), p. 5] describe and [implement](https://github.com/abisee/story-generation-eval/blob/master/metrics.py) it differently from the authors, with division by the total number of _n_-grams. The original authors' [code](https://github.com/YifanZhou95/diversity-promoting-dialogue-system/blob/e4a83359c22299999dab4bc7882c63b1dec51cf3/MMI_antiLM.ipynb) \\[_distinctEval_()] uses tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBRn0y5kq7fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distinct_n(hyp) -> float:\n",
        "    \"\"\"Compute distinct-n for sequence.\"\"\"\n",
        "    df = pd.DataFrame({'Distinct_1': 0,\n",
        "                       'Distinct_2': 0,\n",
        "                       'Distinct_3': 0,\n",
        "                       'Distinct_4': 0}, index=[0])\n",
        "    with open(f\"{hyp}\") as hfile:\n",
        "        lines = hfile.read().splitlines()\n",
        "        d1s = []\n",
        "        d2s = []\n",
        "        d3s = []\n",
        "        d4s = []\n",
        "        for s in lines:\n",
        "            seq = word_tokenize(s)\n",
        "            n = len(seq) or 1\n",
        "            unigrams = set(seq)\n",
        "            bigrams = set(nltk.bigrams(seq)) # distinct bigrams\n",
        "            trigrams = set(nltk.trigrams(seq))\n",
        "            fourgrams = set(nltk.ngrams(seq, 4))\n",
        "            d1s.append(len(unigrams)/n)\n",
        "            d2s.append(len(bigrams)/n)\n",
        "            d3s.append(len(trigrams)/n)\n",
        "            d4s.append(len(fourgrams)/n)\n",
        "    df.Distinct_1 = st.mean(d1s)\n",
        "    df.Distinct_2 = st.mean(d2s)\n",
        "    df.Distinct_3 = st.mean(d3s)\n",
        "    df.Distinct_4 = st.mean(d4s)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9InJdu1J0nA1",
        "colab_type": "text"
      },
      "source": [
        "### Get distinct-1, 2, 3, 4 scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EriwSH4qrANv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddf = distinct_n(hypotheses)\n",
        "display(ddf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJGVQ1SF01eE",
        "colab_type": "text"
      },
      "source": [
        "## Write overlap, embedding, distinct scores to file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB1llSw72gp8",
        "colab_type": "text"
      },
      "source": [
        "Evaluate endings with respect to references, and write endings' scores to evaluations file:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;<small><u>Note</u>: For some cases, temporarily edited *nlgeval*'s \\_init_.py to return individual story results.</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mk7WUCf2bTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Deprecated\n",
        "\n",
        "def write_evals(subtitle: str = \"cn\", metrics_dict: dict = None) -> None:\n",
        "    \"\"\"Old function to write BLEU-1, 2, METEOR, ROUGE-L, CIDEr scores to file for each ending.\"\"\"\n",
        "    \n",
        "    res_dict = {} # Store scores for each ending (generated ending w/ correct ending as reference).\n",
        "    n = None # Number of endings/scores\n",
        "    \n",
        "    for k, v in metrics_dict.items(): # Set up res_dict.\n",
        "        if k not in [\"Bleu_3\", \"Bleu_4\"]:\n",
        "            for i in range(len(v)):\n",
        "                res_dict[i] = {}\n",
        "                \n",
        "    for hyp_num in res_dict.keys(): # For each ending.\n",
        "        for key, values in metrics_dict.items(): # Key = metric, Values = all endings' scores\n",
        "            if not n: # Set number of endings/scores once.\n",
        "                n = len(values)\n",
        "            if key not in [\"Bleu_3\", \"Bleu_4\"]: # Get ending's score for each metric.\n",
        "                res_dict[hyp_num][key] = values[hyp_num]\n",
        "\n",
        "    with open(f\"../evals/{subtitle}_evals.txt\", \"w\") as res_file: # Create eval file, add header of metrics.\n",
        "        line = \"\"\n",
        "        for k in metrics_dict.keys():\n",
        "            if k not in [\"Bleu_3\", \"Bleu_4\"]:\n",
        "                line += f\"{k}\"\n",
        "                if len(line.split(\",\")) != n: # Comma-delimited.\n",
        "                    line += \",\"\n",
        "                else:\n",
        "                    line += \"\\n\" # Newline for next row/ending.\n",
        "        dd = distinct_n\n",
        "        res_file.write(line)\n",
        "\n",
        "    with open(f\"../evals/{subtitle}_evals.txt\", \"a\") as res_file: # Append score for each metric to ending's row.\n",
        "        for hn, scores in res_dict.items():\n",
        "            line = \"\"\n",
        "            for k, v in scores.items(): # For (metric, score) in ending's dictionary of evaluations.\n",
        "                line += f\"{v:0.6}\"\n",
        "                if len(line.split(\",\")) != n:\n",
        "                    line += \",\"\n",
        "                else:\n",
        "                    line += \"\\n\"\n",
        "            res_file.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkGkx-JjrBX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_dist_nlg(fname: str, metrics: dict = None, ddf: dict = None, \n",
        "                   glove=False, skip=False, overlap=False) -> None:\n",
        "    \"\"\"Write BLEU-1, 2, METEOR, ROUGE-L, CIDEr, Distinct-1, 2, 3, 4, embedding scores to file.\"\"\"\n",
        "    if not ddf.empty:\n",
        "        df = mdf.join(ddf)\n",
        "    else:\n",
        "        df = mdf\n",
        "    try:\n",
        "        df = df.drop(columns=['EmbeddingAverageCosineSimilairty'])\n",
        "    except:\n",
        "        pass\n",
        "    if overlap and not glove and not skip:\n",
        "        df.to_csv(f\"evals/{fname}_all_evals.txt\", index=False)\n",
        "    if glove and not overlap:\n",
        "        df.loc[:, 'EmbeddingAverageCosineSimilarity': ].to_csv(f\"evals/{fname}_sg_glove_evals.txt\", index=False)\n",
        "    if skip and not overlap:\n",
        "        pd.DataFrame(data=[df.SkipThoughtCS.values], \n",
        "                     columns=['SkipThoughtCS']).to_csv(f\"evals/{fname}_sg_skip_evals.txt\", \n",
        "                                                       index=False)\n",
        "    if skip and overlap and glove:\n",
        "        df.to_csv(f\"evals/{fname}_o_skip_glove_evals.txt\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EowMTWWrCZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_dist_nlg(fname=fname, metrics=mdf, \n",
        "               glove=False, skip=False, overlap=True,\n",
        "               ddf=ddf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8mRqgTD3C_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddf.to_csv(f\"evals/{fname}_distinct_evals.txt\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}